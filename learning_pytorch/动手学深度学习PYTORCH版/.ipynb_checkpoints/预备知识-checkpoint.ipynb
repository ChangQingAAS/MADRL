{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b169a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import time \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import display\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "import d2lzh as d2l\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3820f274",
   "metadata": {},
   "source": [
    "# 2 预备知识"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deffa88d",
   "metadata": {},
   "source": [
    "## 2.2 数据操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b12363",
   "metadata": {},
   "source": [
    "### 2.2.1创建 `tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6778e718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0286e-38, 1.0653e-38, 1.0194e-38],\n",
       "        [8.4490e-39, 1.0469e-38, 9.3674e-39],\n",
       "        [9.9184e-39, 8.7245e-39, 9.2755e-39],\n",
       "        [8.9082e-39, 9.9184e-39, 8.4490e-39],\n",
       "        [9.6429e-39, 1.0653e-38, 1.0469e-38]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建⼀个5x3的未初始化的 Tensor\n",
    "x = torch.empty(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "600b6be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2475, 0.5785, 0.4930],\n",
       "        [0.8624, 0.1975, 0.8793],\n",
       "        [0.0370, 0.3336, 0.4633],\n",
       "        [0.1396, 0.3209, 0.1564],\n",
       "        [0.2538, 0.8468, 0.1671]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建⼀个5x3的随机初始化的 Tensor :\n",
    "x = torch.rand(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7dd8e1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建⼀个5x3的long型全0的 Tensor :\n",
    "x = torch.zeros(5,3,dtype = torch.long)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e040c229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5000, 2.0000])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接创建\n",
    "x = torch.tensor([5.5,2])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "dd81390e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过现有的tensor插件，会默认重用输入tensor的一些属性，如数据类型等，除非自定义数据类型\n",
    "x = x.new_ones(5, 3, dtype = torch.float64)\n",
    "# 返回的tensor默认具有相同的torch.dtype和torch.device\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "25bb924d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2324, -0.3250,  0.4171],\n",
       "        [ 0.4368,  1.0193,  0.8029],\n",
       "        [-0.1245,  0.3772, -0.1379],\n",
       "        [-1.1019,  2.3555, -0.2890],\n",
       "        [-1.9084, -0.9716, -0.7206]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 制定新的数据类型\n",
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b45d25c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]), torch.Size([5, 3]))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以通过哦shape,size()来获取tensor的形状\n",
    "x.size(),x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76516390",
   "metadata": {},
   "source": [
    "> 注意：返回的torch.Size其实就是一个tuple, 支持所有tuple的操作。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75846381",
   "metadata": {},
   "source": [
    "还有很多函数可以创建`Tensor`，去翻翻官方API就知道了，下表给了一些常用的作参考。\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|Tensor(*sizes)|基础构造函数|\n",
    "|tensor(data,)|类似np.array的构造函数|\n",
    "|ones(*sizes)|全1Tensor|\n",
    "|zeros(*sizes)|全0Tensor|\n",
    "|eye(*sizes)|对角线为1，其他为0|\n",
    "|arange(s,e,step)|从s到e，步长为step|\n",
    "|linspace(s,e,steps)|从s到e，均匀切分成steps份|\n",
    "|rand/randn(*sizes)|均匀/标准分布|\n",
    "|normal(mean,std)/uniform(from,to)|正态分布/均匀分布|\n",
    "|randperm(m)|随机排列|\n",
    "\n",
    "这些创建方法都可以在创建的时候指定数据类型dtype和存放device(cpu/gpu)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48754a6c",
   "metadata": {},
   "source": [
    "### 2.2.2 操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a06bb4",
   "metadata": {},
   "source": [
    "#### 加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c1f1dcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4729,  0.6191,  1.3751],\n",
      "        [ 0.7469,  1.6783,  1.0127],\n",
      "        [ 0.4769,  1.0914, -0.1163],\n",
      "        [-0.6928,  2.6008,  0.2075],\n",
      "        [-1.3178, -0.4104, -0.1003]])\n",
      "tensor([[-0.4729,  0.6191,  1.3751],\n",
      "        [ 0.7469,  1.6783,  1.0127],\n",
      "        [ 0.4769,  1.0914, -0.1163],\n",
      "        [-0.6928,  2.6008,  0.2075],\n",
      "        [-1.3178, -0.4104, -0.1003]])\n",
      "tensor([[-0.4729,  0.6191,  1.3751],\n",
      "        [ 0.7469,  1.6783,  1.0127],\n",
      "        [ 0.4769,  1.0914, -0.1163],\n",
      "        [-0.6928,  2.6008,  0.2075],\n",
      "        [-1.3178, -0.4104, -0.1003]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4729,  0.6191,  1.3751],\n",
       "        [ 0.7469,  1.6783,  1.0127],\n",
       "        [ 0.4769,  1.0914, -0.1163],\n",
       "        [-0.6928,  2.6008,  0.2075],\n",
       "        [-1.3178, -0.4104, -0.1003]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5,3)\n",
    "y = torch.rand(5,3)\n",
    "\n",
    "# 加法形式1：\n",
    "print(x + y )\n",
    "\n",
    "# 加法形式2：\n",
    "print(torch.add(x,y))\n",
    "#还可以指定输出\n",
    "res = torch.empty(5,3)\n",
    "torch.add(x,y,out=res)\n",
    "print(res)\n",
    "\n",
    "# 加法形式3：\n",
    "y.add_(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e08b52",
   "metadata": {},
   "source": [
    "#### 索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a81d264f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.2324,  0.6750,  1.4171]), tensor([-0.2324,  0.6750,  1.4171]))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 索引出来的结果与原数据共享内存，也即修改⼀个，另⼀个会跟着修改\n",
    "y = x[0,:]\n",
    "y += 1\n",
    "\n",
    "y,x[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "02c7bbb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-172-a7743ddc4e52>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-172-a7743ddc4e52>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    y = torch.index_select(x,1,2,*,y)\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "y = torch.index_select(x,1,2,*,y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177f7d87",
   "metadata": {},
   "source": [
    "除了常用的索引选择数据之外，PyTorch还提供了一些高级的选择函数:\n",
    "\n",
    "|函数|\t功能|\n",
    "|:---:|:---:|\n",
    "|index_select(input, dim, index)|在指定维度dim上选取，比如选取某些行、某些列|\n",
    "|masked_select(input, mask)|例子如上，a[a>0]，使用ByteTensor进行选取|\n",
    "|nonzero(input)|\t非0元素的下标|\n",
    "|gather(input, dim, index)|根据index，在dim维度上选取数据，输出的size与index一样|\n",
    "\n",
    "这里不详细介绍，用到了再查官方文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca53c24e",
   "metadata": {},
   "source": [
    "#### 改变形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9c46cd93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]), torch.Size([15]), torch.Size([3, 5]))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view()返回的新tensor与源tensor共享内存\n",
    "# view仅改变了观察方式\n",
    "y = x.view(15)\n",
    "z = x.view(-1,5)\n",
    "\n",
    "x.size(),y.size(),z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4d5d51c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7676,  1.6750,  2.4171],\n",
      "        [ 1.4368,  2.0193,  1.8029],\n",
      "        [ 0.8755,  1.3772,  0.8621],\n",
      "        [-0.1019,  3.3555,  0.7110],\n",
      "        [-0.9084,  0.0284,  0.2794]])\n",
      "tensor([ 0.7676,  1.6750,  2.4171,  1.4368,  2.0193,  1.8029,  0.8755,  1.3772,\n",
      "         0.8621, -0.1019,  3.3555,  0.7110, -0.9084,  0.0284,  0.2794])\n"
     ]
    }
   ],
   "source": [
    "x += 1\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "edf07390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2324,  0.6750,  1.4171],\n",
      "        [ 0.4368,  1.0193,  0.8029],\n",
      "        [-0.1245,  0.3772, -0.1379],\n",
      "        [-1.1019,  2.3555, -0.2890],\n",
      "        [-1.9084, -0.9716, -0.7206]])\n",
      "tensor([ 0.7676,  1.6750,  2.4171,  1.4368,  2.0193,  1.8029,  0.8755,  1.3772,\n",
      "         0.8621, -0.1019,  3.3555,  0.7110, -0.9084,  0.0284,  0.2794])\n"
     ]
    }
   ],
   "source": [
    "# 如果我们想返回⼀个真正新的副本（即不共享内存）\n",
    "#Pytorch还提供了⼀个 reshape() 可以改变形状，但是此函数并不能保证返回的是其拷⻉，所以不推荐使⽤\n",
    "# 推荐先⽤ clone 创造⼀个副本然后再使⽤ view\n",
    "\n",
    "x_cp = x.clone().view(15)\n",
    "x -= 1\n",
    "\n",
    "print(x)\n",
    "print(x_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bff83c",
   "metadata": {},
   "source": [
    "> 使用`clone`还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源`Tensor`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b13d15e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.8057]), 0.8057039380073547)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# item()将一个标量tensor转变成numpy\n",
    "# only one element tensors can be converted to Python scalars\n",
    "x = torch.randn(1)\n",
    "x,x.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bcd783",
   "metadata": {},
   "source": [
    "#### 线性代数\n",
    "另外，PyTorch还支持一些线性函数，这里提一下，免得用起来的时候自己造轮子，具体用法参考官方文档。如下表所示：\n",
    "\n",
    "| 函数\t|功能|\n",
    "|:---:|:---:|\n",
    "|trace|\t对角线元素之和(矩阵的迹)|\n",
    "|diag|\t对角线元素|\n",
    "|triu/tril\t|矩阵的上三角/下三角，可指定偏移量|\n",
    "|mm/bmm\t|矩阵乘法，batch的矩阵乘法|\n",
    "|addmm/addbmm/addmv/addr/baddbmm..|\t矩阵运算|\n",
    "|t|转置|\n",
    "|dot/cross|\t内积/外积|\n",
    "|inverse\t|求逆矩阵|\n",
    "|svd\t|奇异值分解|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e6ad56",
   "metadata": {},
   "source": [
    "### 2.2.3 广播机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83982ec",
   "metadata": {},
   "source": [
    "前面我们看到如何对两个形状相同的`Tensor`做按元素运算。\n",
    "\n",
    "当对两个形状不同的`Tensor`按元素运算时，可能会触发广播（broadcasting）机制：\n",
    "\n",
    "先适当复制元素使这两个`Tensor`形状相同后再按元素运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3c44e781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5017ab3",
   "metadata": {},
   "source": [
    "由于 x 和 y 分别是1⾏2列和3⾏1列的矩阵，如果要计算 x + y ，那么 x 中第⼀⾏的2个元素被⼴播\n",
    "（复制）到了第⼆⾏和第三⾏，⽽ y 中第⼀列的3个元素被⼴播（复制）到了第⼆列。如此，就可以对2\n",
    "个3⾏2列的矩阵按元素相加"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29813de6",
   "metadata": {},
   "source": [
    "### 2.2.4运算的内存开销"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812cf14",
   "metadata": {},
   "source": [
    "前⾯说了，索引、 view 是不会开辟新内存的，⽽像 y = x + y 这样的运算是会新开内存的，然后将 y 指向新内存为了演示这⼀点，我们可以使⽤Python⾃带的 id 函数：如果两个实例的ID⼀致，那么它们所对应的内存地址相同；反之则不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a72d84d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1690813872512, 1690813871872, 640)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "y_id = id(y)\n",
    "y = y + x\n",
    "y_id_ = id(y)\n",
    "\n",
    "y_id,y_id_,y_id - y_id_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b97f5",
   "metadata": {},
   "source": [
    "如果想指定结果到原来的 y 的内存，我们可以使⽤前⾯介绍的索引来进⾏替换操作。在下⾯的例⼦中，我们把 x + y 的结果通过 [:] 写进 y 对应的内存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "390581ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1690815021568, 1690815021568, 0)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "y_id = id(y)\n",
    "y[:] = y + x\n",
    "y_id_ = id(y)\n",
    "\n",
    "y_id,y_id_,y_id - y_id_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a0aa1c",
   "metadata": {},
   "source": [
    "我们还可以使⽤运算符全名函数中的 out 参数或者⾃加运算符 += (也即 add_() )达到上述效果，例如\n",
    "torch.add(x, y, out=y) 和 y += x ( y.add_(x) )。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "42e55469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1690815021952, 1690815021952, True)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "y_id = id(y)\n",
    "torch.add(x,y,out=y)# or y += x, y.add_(x)\n",
    "y_id_ = id(y)\n",
    "\n",
    "y_id,y_id_,y_id == y_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e511b2d",
   "metadata": {},
   "source": [
    "### 2.2.5 TENSOR 和NUMPY相互转换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee41509",
   "metadata": {},
   "source": [
    "Notes: tensor和numpy中的数组也共享内存,（所以他们之间的转换很\n",
    "快），改变其中⼀个时另⼀个也会改变！！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac4799e",
   "metadata": {},
   "source": [
    "#### numpy中的array转换成tensor\n",
    "需要注意的是，此⽅法总是会进⾏数据拷⻉（就会消耗更多的时间和空间），所以返回的 Tensor 和原来的数\n",
    "据不再共享内存。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0cf96de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 2., 2.]), tensor([2., 2., 2.], dtype=torch.float64))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones(3)\n",
    "b = torch.from_numpy(a)\n",
    "\n",
    "a += 1\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f420d9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 2., 2.]), tensor([2., 2., 2.], dtype=torch.float64))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 法2\n",
    "b = torch.tensor(a)\n",
    "\n",
    "a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d354c",
   "metadata": {},
   "source": [
    "所有在CPU上的 Tensor （除了 CharTensor ）都⽀持与NumPy数组相互转换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f339b0c",
   "metadata": {},
   "source": [
    "#### tensor转numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ace5ed1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2.]), array([2., 2.], dtype=float32))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(2)\n",
    "b = a.numpy()\n",
    "\n",
    "a += 1\n",
    "\n",
    "a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac6c1a",
   "metadata": {},
   "source": [
    "### 2.2.6 TENSOR ON GPU\n",
    "用方法`to()`可以将`Tensor`在CPU和GPU（需要硬件支持）之间相互移动。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e2eda1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下代码只有在PyTorch GPU版本上才会执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f01593",
   "metadata": {},
   "source": [
    "## 2.3 ⾃动求梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78d20c9",
   "metadata": {},
   "source": [
    "在深度学习中，我们经常需要对函数求梯度（gradient）。PyTorch提供的autograd 包能够根据输⼊\n",
    "和前向传播过程⾃动构建计算图，并执⾏反向传播。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83963734",
   "metadata": {},
   "source": [
    "### 2.3.1 概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5828e31b",
   "metadata": {},
   "source": [
    "上一节介绍的`Tensor`是这个包的核心类，如果将其属性`.requires_grad`设置为`True`，它将开始追踪(track)在其上的所有操作（这样就可以利用链式法则进行梯度传播了）。完成计算后，可以调用`.backward()`来完成所有梯度计算。此`Tensor`的梯度将累积到`.grad`属性中。\n",
    "> 注意在`y.backward()`时，如果`y`是标量，则不需要为`backward()`传入任何参数；否则，需要传入一个与`y`同形的`Tensor`。解释见 2.3.2 节。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a4d5d5",
   "metadata": {},
   "source": [
    "如果不想要被继续追踪，可以调⽤ .detach() 将其从追踪记录中分离出来，这样就可以防⽌将来的计算被追踪，这样梯度就传不过去了。此外，还可以⽤ with torch.no_grad() 将不想被追踪的操作代码块包裹起来，这种⽅法在评估模型的时候很常⽤，因为在评估模型时，我们并不需要计算可训练参数（ requires_grad=True ）的梯度。\n",
    "\n",
    "Function 是另外⼀个很᯿要的类。 Tensor 和 Function 互相结合就可以构建⼀个记录有整个计算过程的有向⽆环图（DAG）。每个 Tensor 都有⼀个 .grad_fn 属性，该属性即创建该 Tensor 的Function , 就是说该 Tensor 是不是通过某些运算得到的，若是，则 grad_fn 返回⼀个与这些运算相关的对象，否则是None。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916e0427",
   "metadata": {},
   "source": [
    "### 2.3.2 `Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e0f8e04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1.],\n",
       "         [1., 1.]], requires_grad=True),\n",
       " None)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 即把x当做变量\n",
    "x = torch.ones(2,2,requires_grad = True)\n",
    "x,x.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c16d431e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 3.],\n",
       "         [3., 3.]], grad_fn=<AddBackward0>),\n",
       " <AddBackward0 at 0x189ac97ca60>)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 做一下运算\n",
    "\n",
    "y = x +2\n",
    "y,y.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9259b986",
   "metadata": {},
   "source": [
    "注意x是直接创建的，所以它没有 grad_fn , ⽽y是通过⼀个加法操作创建的，所以它有⼀个为\n",
    "<AddBackward> 的 grad_fn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "2418af72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "# 像x这种直接创建的称为叶⼦节点，叶⼦节点对应的 grad_fn 是 None 。\n",
    "print(x.is_leaf,y.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5e7c5561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "<MulBackward0 object at 0x00000189ACF92D90>\n",
      "\n",
      "tensor(27., grad_fn=<MeanBackward0>)\n",
      "<MeanBackward0 object at 0x00000189AC5F6370>\n"
     ]
    }
   ],
   "source": [
    "# 可以从grad_fn看出本次的操作 or 运算\n",
    "z = y ** 2 * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z)\n",
    "print(z.grad_fn)\n",
    "print()\n",
    "print(out)\n",
    "print(out.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e93691c",
   "metadata": {},
   "source": [
    "通过`.requires_grad_()`来用in-place的方式改变`requires_grad`属性：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a5a5d837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x00000189AC5D3A30>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2) # 缺失情况下默认 requires_grad = False\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad) # False\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad) # True\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3204b8",
   "metadata": {},
   "source": [
    "### 2.3.3 梯度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "bb5f521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  因为 out 是⼀个标量，所以调⽤ backward() 时不需要指定求导变量\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b82099",
   "metadata": {},
   "source": [
    " 我们来看看`out`关于`x`的梯度 $\\frac{d(out)}{dx}$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b54f7c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c00a643",
   "metadata": {},
   "source": [
    "我们令`out`为 $o$ , 因为\n",
    "$$\n",
    "o=\\frac14\\sum_{i=1}^4z_i=\\frac14\\sum_{i=1}^43(x_i+2)^2\n",
    "$$\n",
    "所以\n",
    "$$\n",
    "\\frac{\\partial{o}}{\\partial{x_i}}\\bigr\\rvert_{x_i=1}=\\frac{9}{2}=4.5\n",
    "$$\n",
    "所以上面的输出是正确的。\n",
    "\n",
    "数学上，如果有一个函数值和自变量都为向量的函数 $\\vec{y}=f(\\vec{x})$, 那么 $\\vec{y}$ 关于 $\\vec{x}$ 的梯度就是一个雅可比矩阵（Jacobian matrix）:\n",
    "$$\n",
    "J=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\n",
    "$$\n",
    "而``torch.autograd``这个包就是用来计算一些雅克比矩阵的乘积的。例如，如果 $v$ 是一个标量函数的 $l=g\\left(\\vec{y}\\right)$ 的梯度：\n",
    "$$\n",
    "v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)\n",
    "$$\n",
    "那么根据链式法则我们有 $l$ 关于 $\\vec{x}$ 的雅克比矩阵就为:\n",
    "$$\n",
    "v J=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right) \\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial x_{1}} & \\cdots & \\frac{\\partial l}{\\partial x_{n}}\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "注意：grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ade4b009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.5000, 5.5000],\n",
      "        [5.5000, 5.5000]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#再来一次反向传播，注意grad是累加的\n",
    "\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "\n",
    "out3 = x.sum()\n",
    "x.grad.data.zero_()\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5148f0df",
   "metadata": {},
   "source": [
    "\n",
    "> 现在我们解释2.3.1节留下的问题，为什么在`y.backward()`时，如果`y`是标量，则不需要为`backward()`传入任何参数；否则，需要传入一个与`y`同形的`Tensor`?\n",
    "简单来说就是为了避免向量（甚至更高维张量）对张量求导，而转换成标量对张量求导。举个例子，假设形状为 `m x n` 的矩阵 X 经过运算得到了 `p x q` 的矩阵 Y，Y 又经过运算得到了 `s x t` 的矩阵 Z。那么按照前面讲的规则，dZ/dY 应该是一个 `s x t x p x q` 四维张量，dY/dX 是一个 `p x q x m x n`的四维张量。问题来了，怎样反向传播？怎样将两个四维张量相乘？？？这要怎么乘？？？就算能解决两个四维张量怎么乘的问题，四维和三维的张量又怎么乘？导数的导数又怎么求，这一连串的问题，感觉要疯掉…… \n",
    "为了避免这个问题，我们**不允许张量对张量求导，只允许标量对张量求导，求导结果是和自变量同形的张量**。所以必要时我们要**把张量通过将所有张量的元素加权求和的方式转换为标量**，举个例子，假设`y`由自变量`x`计算而来，`w`是和`y`同形的张量，则`y.backward(w)`的含义是：先计算`l = torch.sum(y * w)`，则`l`是个标量，然后求`l`对自变量`x`的导数。\n",
    "[参考](https://zhuanlan.zhihu.com/p/29923090)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b25e822f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 4.],\n",
       "        [6., 8.]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only Tensors of floating point and complex dtype can require gradients\n",
    "x = torch.tensor([1.,2.,3.,4.],requires_grad=True)\n",
    "y = 2 * x\n",
    "z = y.view(2,2)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8538d4",
   "metadata": {},
   "source": [
    "现在 z 不是⼀个标量，所以在调⽤ backward 时需要传⼊⼀个和z同形的权重向量进⾏加权求和得到\n",
    "⼀个标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "119fc904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000, 0.2000, 0.0200, 0.0020])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([[1.0, 0.1], [0.01, 0.001]], dtype=torch.float)\n",
    "z.backward(v)\n",
    "print(x.grad)\n",
    "# 注意， x.grad 是和 x 同形的张量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef7195e",
   "metadata": {},
   "source": [
    "**为什么**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1df095a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for question\n",
    "l = torch.sum(v * z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "2d641136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(1., grad_fn=<PowBackward0>) True\n",
      "tensor(1.) False\n",
      "tensor(2., grad_fn=<AddBackward0>) True\n",
      "tensor(2.)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-196-d07e305001b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 只算了y1 part\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0my2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 因为requires_grad = false，所以报错\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# 再来卡看中断梯度的例子：\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y1 = x ** 2\n",
    "with torch.no_grad():\n",
    "    y2 = x ** 3\n",
    "y3 = y1 + y2\n",
    "\n",
    "print(x.requires_grad)\n",
    "print(y1, y1.requires_grad)\n",
    "print(y2, y2.requires_grad)\n",
    "print(y3, y3.requires_grad)\n",
    "\n",
    "y3.backward()\n",
    "print(x.grad) # 只算了y1 part\n",
    "\n",
    "y2.backward() # 因为requires_grad = false，所以报错"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f74971",
   "metadata": {},
   "source": [
    "此外，如果我们想要修改 tensor 的数值，但是⼜不希望被 autograd 记录（即不会影响反向传播），\n",
    "那么我么可以对 tensor.data 进⾏操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68473c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1,requires_grad=True)\n",
    "print(x.data) # 还是⼀个tensor\n",
    "print(x.data.requires_grad) # 但是已经是独⽴于计算图之外\n",
    "y = 2 * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "y.backward()\n",
    "print(x) # 更改data的值也会影响tensor的值\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d17a79e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "243.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
