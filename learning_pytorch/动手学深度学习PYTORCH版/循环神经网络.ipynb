{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%matplotlib inline\r\n",
    "import random\r\n",
    "import time \r\n",
    "import math\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from torch.nn import init\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from IPython import display\r\n",
    "from collections import OrderedDict\r\n",
    "import sys\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "from mpl_toolkits import mplot3d\r\n",
    "import zipfile\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch import nn, optim, utils\r\n",
    "import d2lzh as d2l\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.1语言模型\r\n",
    "语言模型（language model）是自然语言处理的重要技术。自然语言处理中最常见的数据是文本数据。我们可以把一段自然语言文本看作一段离散的时间序列。假设一段长度为$T$的文本中的词依次为$w_1, w_2, \\ldots, w_T$，那么在离散的时间序列中，$w_t$（$1 \\leq t \\leq T$）可看作在时间步（time step）$t$的输出或标签。给定一个长度为$T$的词的序列$w_1, w_2, \\ldots, w_T$，语言模型将计算该序列的概率：\r\n",
    "\r\n",
    "$$P(w_1, w_2, \\ldots, w_T).$$\r\n",
    "语言模型可用于提升语音识别和机器翻译的性能。例如，在语音识别中，给定一段“厨房里食油用完了”的语音，有可能会输出“厨房里食油用完了”和“厨房里石油用完了”这两个读音完全一样的文本序列。如果语言模型判断出前者的概率大于后者的概率，我们就可以根据相同读音的语音输出“厨房里食油用完了”的文本序列。在机器翻译中，如果对英文“you go first”逐词翻译成中文的话，可能得到“你走先”“你先走”等排列方式的文本序列。如果语言模型判断出“你先走”的概率大于其他排列方式的文本序列的概率，我们就可以把“you go first”翻译成“你先走”。\r\n",
    "\r\n",
    "### 语言模型的计算\r\n",
    "既然语言模型很有用，那该如何计算它呢？假设序列$w_1, w_2, \\ldots, w_T$中的每个词是依次生成的，我们有\r\n",
    "\r\n",
    "$$P(w_1, w_2, \\ldots, w_T) = \\prod_{t=1}^T P(w_t \\mid w_1, \\ldots, w_{t-1}).$$\r\n",
    "例如，一段含有4个词的文本序列的概率\r\n",
    "\r\n",
    "$$P(w_1, w_2, w_3, w_4) =  P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_1, w_2) P(w_4 \\mid w_1, w_2, w_3).$$\r\n",
    "为了计算语言模型，我们需要计算词的概率，以及一个词在给定前几个词的情况下的条件概率，即语言模型参数。设训练数据集为一个大型文本语料库，如维基百科的所有条目。词的概率可以通过该词在训练数据集中的相对词频来计算。例如，$P(w_1)$可以计算为$w_1$在训练数据集中的词频（词出现的次数）与训练数据集的总词数之比。因此，根据条件概率定义，一个词在给定前几个词的情况下的条件概率也可以通过训练数据集中的相对词频计算。例如，$P(w_2 \\mid w_1)$可以计算为$w_1, w_2$两词相邻的频率与$w_1$词频的比值，因为该比值即$P(w_1, w_2)$与$P(w_1)$之比；而$P(w_3 \\mid w_1, w_2)$同理可以计算为$w_1$、$w_2$和$w_3$三词相邻的频率与$w_1$和$w_2$ **两词相邻**的频率的比值。以此类推。\r\n",
    "\r\n",
    "note: 这里的相邻应该是有顺序的\r\n",
    "\r\n",
    "### $n$元语法\r\n",
    "当序列长度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。$n$元语法通过马尔可夫假设（虽然并不一定成立）简化了语言模型的计算。这里的马尔可夫假设是指一个词的出现只与前面$n$个词相关，即$n$阶马尔可夫链（Markov chain of order $n$）。如果$n=1$，那么有$P(w_3 \\mid w_1, w_2) = P(w_3 \\mid w_2)$。如果基于$n-1$阶马尔可夫链，我们可以将语言模型改写为\r\n",
    "\r\n",
    "$$P(w_1, w_2, \\ldots, w_T) \\approx \\prod_{t=1}^T P(w_t \\mid w_{t-(n-1)}, \\ldots, w_{t-1}) .$$\r\n",
    "以上也叫$n$元语法（$n$-grams）。它是基于$n - 1$阶马尔可夫链的概率语言模型。当$n$分别为1、2和3时，我们将其分别称作一元语法（unigram）、二元语法（bigram）和三元语法（trigram）。例如，长度为4的序列$w_1, w_2, w_3, w_4$在一元语法、二元语法和三元语法中的概率分别为\r\n",
    "\r\n",
    "$$\r\n",
    "\\begin{aligned}\r\n",
    "P(w_1, w_2, w_3, w_4) &amp;=  P(w_1) P(w_2) P(w_3) P(w_4) ,\\\\\r\n",
    "P(w_1, w_2, w_3, w_4) &amp;=  P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_2) P(w_4 \\mid w_3) ,\\\\\r\n",
    "P(w_1, w_2, w_3, w_4) &amp;=  P(w_1) P(w_2 \\mid w_1) P(w_3 \\mid w_1, w_2) P(w_4 \\mid w_2, w_3) .\r\n",
    "\\end{aligned}\r\n",
    "$$\r\n",
    "当$n$较小时，$n$元语法往往并不准确。例如，在一元语法中，由三个词组成的句子“你走先”和“你先走”的概率是一样的。然而，当$n$较大时，$n$元语法需要计算并存储大量的词频和多词相邻频率。\r\n",
    "\r\n",
    "那么，有没有方法在语言模型中更好地平衡以上这两点呢？我们将在本章探究这样的方法。\r\n",
    "\r\n",
    "### 小结\r\n",
    "- 语言模型是自然语言处理的重要技术。\r\n",
    "- $N$元语法是基于$n-1$阶马尔可夫链的概率语言模型，其中$n$权衡了计算复杂度和模型准确性。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.2循环神经网络\r\n",
    "上一节介绍的$n$元语法中，时间步$t$的词$w_t$基于前面所有词的条件概率只考虑了最近时间步的$n-1$个词。如果要考虑比$t-(n-1)$更早时间步的词对$w_t$的可能影响，我们需要增大$n$。但这样模型参数的数量将随之呈指数级增长（可参考上一节的练习）。\r\n",
    "\r\n",
    "本节将介绍循环神经网络。它并非刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。首先我们回忆一下前面介绍过的多层感知机，然后描述如何添加隐藏状态来将它变成循环神经网络\r\n",
    "\r\n",
    "### 不含隐藏状态的神经网络\r\n",
    "让我们考虑一个含单隐藏层的多层感知机。给定样本数为$n$、输入个数（特征数或特征向量维度）为$d$的小批量数据样本$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times d}$。设隐藏层的激活函数为$\\phi$，那么隐藏层的输出$\\boldsymbol{H} \\in \\mathbb{R}^{n \\times h}$计算为\r\n",
    "\r\n",
    "$$\\boldsymbol{H} = \\phi(\\boldsymbol{X} \\boldsymbol{W}_{xh} + \\boldsymbol{b}_h),$$\r\n",
    "其中隐藏层权重参数$\\boldsymbol{W}_{xh} \\in \\mathbb{R}^{d \\times h}$，隐藏层偏差参数 $\\boldsymbol{b}_h \\in \\mathbb{R}^{1 \\times h}$，$h$为隐藏单元个数。上式相加的两项形状不同，因此将按照广播机制相加（参见“数据操作”一节）。把隐藏变量$\\boldsymbol{H}$作为输出层的输入，且设输出个数为$q$（如分类问题中的类别数），输出层的输出为\r\n",
    "\r\n",
    "$$\\boldsymbol{O} = \\boldsymbol{H} \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q,$$\r\n",
    "其中输出变量$\\boldsymbol{O} \\in \\mathbb{R}^{n \\times q}$, 输出层权重参数$\\boldsymbol{W}_{hq} \\in \\mathbb{R}^{h \\times q}$, 输出层偏差参数$\\boldsymbol{b}_q \\in \\mathbb{R}^{1 \\times q}$。如果是分类问题，我们可以使用$\\text{softmax}(\\boldsymbol{O})$来计算输出类别的概率分布\r\n",
    "\r\n",
    "### 含隐藏状态的循环神经网络\r\n",
    "现在我们考虑输入数据存在时间相关性的情况。假设$\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}$是序列中时间步$t$的小批量输入，$\\boldsymbol{H}_t  \\in \\mathbb{R}^{n \\times h}$是该时间步的隐藏变量。与多层感知机不同的是，这里我们保存上一时间步的隐藏变量$\\boldsymbol{H}_{t-1}$，并引入一个新的权重参数$\\boldsymbol{W}_{hh} \\in \\mathbb{R}^{h \\times h}$，该参数用来描述在当前时间步如何使用上一时间步的隐藏变量。具体来说，时间步$t$的隐藏变量的计算由当前时间步的输入和上一时间步的隐藏变量共同决定：\r\n",
    "\r\n",
    "$$\\boldsymbol{H}_t = \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh}  + \\boldsymbol{b}_h).$$\r\n",
    "与多层感知机相比，我们在这里添加了$\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh}$一项。由上式中相邻时间步的隐藏变量$\\boldsymbol{H}_t$和$\\boldsymbol{H}_{t-1}$之间的关系可知，这里的隐藏变量能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。因此，该隐藏变量也称为隐藏状态。由于隐藏状态在当前时间步的定义使用了上一时间步的隐藏状态，上式的计算是循环的。使用循环计算的网络即循环神经网络（recurrent neural network）。\r\n",
    "\r\n",
    "循环神经网络有很多种不同的构造方法。含上式所定义的隐藏状态的循环神经网络是极为常见的一种。若无特别说明，本章中的循环神经网络均基于上式中隐藏状态的循环计算。在时间步$t$，输出层的输出和多层感知机中的计算类似：\r\n",
    "\r\n",
    "$$\\boldsymbol{O}_t = \\boldsymbol{H}_t \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q.$$\r\n",
    "循环神经网络的参数包括隐藏层的权重$\\boldsymbol{W}_{xh} \\in \\mathbb{R}^{d \\times h}$、$\\boldsymbol{W}_{hh} \\in \\mathbb{R}^{h \\times h}$和偏差 $\\boldsymbol{b}_h \\in \\mathbb{R}^{1 \\times h}$，以及输出层的权重$\\boldsymbol{W}_{hq} \\in \\mathbb{R}^{h \\times q}$和偏差$\\boldsymbol{b}_q \\in \\mathbb{R}^{1 \\times q}$。值得一提的是，即便在不同时间步，循环神经网络也始终使用这些模型参数。因此，循环神经网络模型参数的数量不随时间步的增加而增长。\r\n",
    "\r\n",
    "图6.1展示了循环神经网络在3个相邻时间步的计算逻辑。在时间步$t$，隐藏状态的计算可以看成是将输入$\\boldsymbol{X}_t$和前一时间步隐藏状态$\\boldsymbol{H}_{t-1}$连结后输入一个激活函数为$\\phi$的全连接层。该全连接层的输出就是当前时间步的隐藏状态$\\boldsymbol{H}_t$，且模型参数为$\\boldsymbol{W}_{xh}$与$\\boldsymbol{W}_{hh}$的连结，偏差为$\\boldsymbol{b}_h$。当前时间步$t$的隐藏状态$\\boldsymbol{H}_t$将参与下一个时间步$t+1$的隐藏状态$\\boldsymbol{H}_{t+1}$的计算，并输入到当前时间步的全连接输出层。\r\n",
    "\r\n",
    "![](./img/rnn.svg)\r\n",
    "我们刚刚提到，隐藏状态中$\\boldsymbol{X}_t \\boldsymbol{W}_{xh} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh}$的计算等价于$\\boldsymbol{X}_t$与$\\boldsymbol{H}_{t-1}$连结后的矩阵乘以$\\boldsymbol{W}_{xh}$与$\\boldsymbol{W}_{hh}$连结后的矩阵。接下来，我们用一个具体的例子来验证这一点。首先，我们构造矩阵X、W_xh、H和W_hh，它们的形状分别为(3, 1)、(1, 4)、(3, 4)和(4, 4)。将X与W_xh、H与W_hh分别相乘，再把两个乘法运算的结果相加，得到形状为(3, 4)的矩阵。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "X, W_xh = torch.randn(3,1), torch.randn(1,4)\r\n",
    "H, W_hh = torch.randn(3,4), torch.randn(4,4)\r\n",
    "\r\n",
    "print(torch.mm(X,W_xh) + torch.mm(H,W_hh))\r\n",
    "\r\n",
    "# 另一种写法\r\n",
    "torch.mm(torch.cat((X,H), dim = 1), torch.cat((W_xh, W_hh), dim = 0))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 4.4120, -2.3057,  2.7529,  0.8539],\n",
      "        [-0.6899,  0.7637, -0.6249,  0.6287],\n",
      "        [-3.3006,  2.1449, -2.3104,  2.5815]])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 4.4120, -2.3057,  2.7529,  0.8539],\n",
       "        [-0.6899,  0.7637, -0.6249,  0.6287],\n",
       "        [-3.3006,  2.1449, -2.3104,  2.5815]])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "将矩阵X和H按列（维度1）连结，连结后的矩阵形状为(3, 5)。可见，连结后矩阵在维度1的长度为矩阵X和H在维度1的长度之和（$1+4$）。然后，将矩阵W_xh和W_hh按行（维度0）连结，连结后的矩阵形状为(5, 4)。最后将两个连结后的矩阵相乘，得到与上面代码输出相同的形状为(3, 4)的矩阵。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 基于字符级循环神经网络的语言模型\r\n",
    "\r\n",
    "最后我们介绍如何应用循环神经网络来构建一个语言模型。设小批量中样本数为1，文本序列为“想”“要”“有”“直”“升”“机”。图6.2演示了如何使用循环神经网络基于当前和过去的字符来预测下一个字符。在训练时，我们对每个时间步的输出层输出使用softmax运算，然后使用交叉熵损失函数来计算它与标签的误差。在图6.2中，由于隐藏层中隐藏状态的循环计算，时间步3的输出$\\boldsymbol{O}_3$取决于文本序列“想”“要”“有”。 由于训练数据中该序列的下一个词为“直”，时间步3的损失将取决于该时间步基于序列“想”“要”“有”生成下一个词的概率分布与该时间步的标签“直”。\r\n",
    "\r\n",
    "![](https://raw.githubusercontent.com/sangyx/d2l-torch/31b757807e3ff637436765c1dff09315d97dcff8/img/rnn-train.svg)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 小结\r\n",
    "- 使用循环计算的网络即循环神经网络。\r\n",
    "- 循环神经网络的隐藏状态可以捕捉截至当前时间步的序列的历史信息。\r\n",
    "- 循环神经网络模型参数的数量不随时间步的增加而增长。\r\n",
    "- 可以基于字符级循环神经网络来创建语言模型。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 语言模型数据集（周杰伦专辑歌词）\r\n",
    "本节将介绍如何预处理一个语言模型数据集，并将其转换成字符级循环神经网络所需要的输入格式。为此，我们收集了周杰伦从第一张专辑《Jay》到第十张专辑《跨时代》中的歌词，并在后面几节里应用循环神经网络来训练一个语言模型。当模型训练好后，我们就可以用这个模型来创作歌词。\r\n",
    "### 读取数据集¶\r\n",
    "首先读取这个数据集，看看前40个字符是什么样的。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "with zipfile.ZipFile(\"./Datasets/jaychou_lyrics.txt.zip\") as zin:\r\n",
    "    with zin.open(\"jaychou_lyrics.txt\") as f:\r\n",
    "        corpus_chars = f.read().decode(\"utf-8\")\r\n",
    "    # print(len(corpus_chars))\r\n",
    "corpus_chars[:40]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'想要有直升机\\n想要和你飞到宇宙去\\n想要和你融化在一起\\n融化在宇宙里\\n我每天每天每'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# 为了打印方便，我们把换行符替换成空格，然后仅使用前1万个字符来训练模型。\r\n",
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace(\"\\r\", \" \")\r\n",
    "corpus_chars = corpus_chars[:10000]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 建立字符索引\r\n",
    "\r\n",
    "将每个字符映射成一个从0开始的连续整数，又称索引，来方便之后的数据处理。为了得到索引，我们将数据集里所有不同字符取出来，然后将其逐一映射到索引来构造词典。接着，打印vocab_size，即词典中不同字符的个数，又称词典大小。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "idx_to_char = list(set(corpus_chars))\r\n",
    "char_to_idx = dict([(char, i ) for i,char in enumerate(idx_to_char)])\r\n",
    "vocab_size = len(char_to_idx)\r\n",
    "# char_to_idx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# 将训练数据集中每个字符转化为索引，并打印前20个字符及其对应的索引。\r\n",
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\r\n",
    "# len(corpus_indices)\r\n",
    "\r\n",
    "sample = corpus_indices[:20]\r\n",
    "print('chars:', \"\".join([idx_to_char[idx] for idx in sample]))\r\n",
    "print(\"indoces:\",sample)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "chars: 想要有直升机 想要和你飞到宇宙去 想要和\n",
      "indoces: [972, 209, 494, 560, 115, 725, 125, 972, 209, 35, 840, 870, 502, 333, 542, 323, 125, 972, 209, 35]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们将以上代码封装在d2ltorch包里的load_data_jay_lyrics函数中，以方便后面章节调用。调用该函数后会依次得到corpus_indices、char_to_idx、idx_to_char和vocab_size这4个变量。\r\n",
    "\r\n",
    "### 时序数据的采样\r\n",
    "\r\n",
    "在训练中我们需要每次随机读取小批量样本和标签。与之前章节的实验数据不同的是，时序数据的一个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想”“要”“有”“直”“升”。该样本的标签序列为这些字符分别在训练集中的下一个字符，即“要”“有”“直”“升”“机”。我们有两种方式对时序数据进行采样，分别是随机采样和相邻采样。\r\n",
    "\r\n",
    "#### 随机采样\r\n",
    "下面的代码每次从数据里随机采样一个小批量。其中批量大小batch_size指每个小批量的样本数，num_steps为每个样本所包含的时间步数。 在随机采样中，每个样本是原始序列上任意截取的一段序列。相邻的两个随机小批量在原始序列上的位置不一定相毗邻。因此，我们无法用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态。在训练模型时，每次随机采样前都需要重新初始化隐藏状态。\r\n",
    "？？？？"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def data_iter_random(corpus_indices, batch_size, num_steps, device = 'cpu'):\r\n",
    "\r\n",
    "    # 减1是因为输出的索引是相应输入的索引加1\r\n",
    "    num_examples = (len(corpus_indices) - 1 ) // num_steps\r\n",
    "    epoch_size = num_examples // batch_size\r\n",
    "    example_indices = list(range(num_examples))\r\n",
    "    random.shuffle(example_indices)\r\n",
    "\r\n",
    "    # 返回从pos开始的长为num_steps的序列\r\n",
    "    def _data(pos):\r\n",
    "        return corpus_indices[pos : pos+ num_steps]\r\n",
    "\r\n",
    "    for i in range(epoch_size):\r\n",
    "        # 每次读取batch_size个随机样本\r\n",
    "        i = i * batch_size\r\n",
    "        batch_indices = example_indices[i : i + batch_size]\r\n",
    "        X = torch.Tensor([_data(j * num_steps) for j in batch_indices]).to(device)\r\n",
    "        Y = torch.Tensor([_data(j * num_steps + 1) for j in batch_indices]).to(device)\r\n",
    "        yield X, Y\r\n",
    "\r\n",
    "# 每次num_steps -> batchs  -> epochs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# 随便写一个seq， 验证一下\r\n",
    "my_seq = list(range(30))\r\n",
    "for X, Y in data_iter_random(my_seq, batch_size=2, num_steps=6):\r\n",
    "    print('X', X, '\\nY:', Y, '\\n')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
      "        [18., 19., 20., 21., 22., 23.]]) \n",
      "Y: tensor([[ 1.,  2.,  3.,  4.,  5.,  6.],\n",
      "        [19., 20., 21., 22., 23., 24.]]) \n",
      "\n",
      "X tensor([[12., 13., 14., 15., 16., 17.],\n",
      "        [ 6.,  7.,  8.,  9., 10., 11.]]) \n",
      "Y: tensor([[13., 14., 15., 16., 17., 18.],\n",
      "        [ 7.,  8.,  9., 10., 11., 12.]]) \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 相邻采样\r\n",
    "\r\n",
    "\r\n",
    "除对原始序列做随机采样之外，我们还可以令相邻的两个随机小批量在原始序列上的位置相毗邻。这时候，我们就可以用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态，从而使下一个小批量的输出也取决于当前小批量的输入，并如此循环下去。这对实现循环神经网络造成了两方面影响：一方面， 在训练模型时，我们只需在每一个迭代周期开始时初始化隐藏状态；另一方面，当多个相邻小批量通过传递隐藏状态串联起来时，模型参数的梯度计算将依赖所有串联起来的小批量序列。同一迭代周期中，随着迭代次数的增加，梯度的计算开销会越来越大。 为了使模型参数的梯度计算只依赖一次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来。我们将在[“循环神经网络的从零开始实现”] 一节的实现中了解这种处理方式。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, device = 'cpu'):\r\n",
    "    corpus_indices = torch.Tensor(corpus_indices).to(device)\r\n",
    "    data_len = len(corpus_indices)\r\n",
    "    batch_len = data_len // batch_size\r\n",
    "    indices = corpus_indices[0: batch_size * batch_len].reshape(batch_size, batch_len)\r\n",
    "    epoch_size = (batch_len - 1) // num_steps   \r\n",
    "    for i in range(epoch_size):\r\n",
    "        i = i * num_steps\r\n",
    "        X = indices[:, i : i+ num_steps]\r\n",
    "        Y = indices[:, i+1 :  i + num_steps +1]\r\n",
    "        yield X,Y\r\n",
    "\r\n",
    "\r\n",
    "## 为什么要设计成这种每个batch大小 = 回合数 * 每个样本的大小 的形式呢？\r\n",
    "## 就是说为什么要先除batch而不是先除epoch呢，是因为所传参数的限制吗"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# 同样的设置下，打印相邻采样每次读取的小批量样本的输入X和标签Y。\r\n",
    "# 相邻的两个随机小批量在原始序列上的位置相毗邻。\r\n",
    "for X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=6):\r\n",
    "    print('X', X, '\\nY:', Y, '\\n')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
      "        [15., 16., 17., 18., 19., 20.]]) \n",
      "Y: tensor([[ 1.,  2.,  3.,  4.,  5.,  6.],\n",
      "        [16., 17., 18., 19., 20., 21.]]) \n",
      "\n",
      "X tensor([[ 6.,  7.,  8.,  9., 10., 11.],\n",
      "        [21., 22., 23., 24., 25., 26.]]) \n",
      "Y: tensor([[ 7.,  8.,  9., 10., 11., 12.],\n",
      "        [22., 23., 24., 25., 26., 27.]]) \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 小结\r\n",
    "\r\n",
    "- 时序数据采样方式包括随机采样和相邻采样。使用这两种方式的循环神经网络训练在实现上略有不同。（即算法设计要适配与datasets\r\n",
    "\r\n",
    "### 练习\r\n",
    "- 如果我们希望一个序列样本是一个完整的句子，这会给小批量采样带来什么样的问题？\r\n",
    "  - 改变了num_steps的大小，数据集内的样本大小不统一"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "\r\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else\r\n",
    "'cpu')\r\n",
    "\r\n",
    "# with zipfile.ZipFile('./Datasets/jaychou_lyrics.txt.zip') as zin:\r\n",
    "#         with zin.open('jaychou_lyrics.txt') as f:\r\n",
    "#             corpus_chars = f.read().decode('utf-8')\r\n",
    "\r\n",
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### one-hot向量\r\n",
    "\r\n",
    "为了将词表示成向量输入到神经网络，一个简单的办法是使用one-hot向量。假设词典中不同字符的数量为$N$（即词典大小vocab_size），每个字符已经同一个从0到$N-1$的连续整数值索引一一对应。如果一个字符的索引是整数$i$, 那么我们创建一个全0的长为$N$的向量，并将其位置为$i$的元素设成1。该向量就是对原字符的one-hot向量。下面分别展示了索引为0和2的one-hot向量，向量长度等于词典大小。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "def one_hot(idx, size, device = 'cpu'):\r\n",
    "    idx = idx.long()\r\n",
    "    batch_size = idx.size(0)\r\n",
    "    # print(batch_size)\r\n",
    "    # print(idx.size())\r\n",
    "    index = idx.reshape(-1,1)\r\n",
    "    # print(index.size())\r\n",
    "    return torch.zeros(batch_size,size,dtype=torch.float32).to(device).scatter_(dim = 1, index = index, value = 1)\r\n",
    "\r\n",
    "one_hot(torch.LongTensor([0,2]), vocab_size)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们每次采样的小批量的形状是(批量大小, 时间步数)。下面的函数将这样的小批量变换成数个可以输入进网络的形状为(批量大小, 词典大小)的矩阵，矩阵个数等于时间步数。也就是说，时间步$t$的输入为$\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}$，其中$n$为批量大小，$d$为输入个数，即one-hot向量长度（词典大小）"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "def to_onehot(X, size, device = 'cpu'):\r\n",
    "    return [one_hot(X[:,i], size, device) for i in range(X.shape[1])]\r\n",
    "\r\n",
    "X = torch.arange(10).reshape(2,5)\r\n",
    "inputs = to_onehot(X, vocab_size)\r\n",
    "\r\n",
    "print(inputs[0].shape)\r\n",
    "len(inputs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2, 1027])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 初始化模型参数\r\n",
    "接下来，我们初始化模型参数。隐藏单元个数 num_hiddens是一个超参数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "num_inputs = vocab_size\r\n",
    "num_hiddens = 256\r\n",
    "num_outputs = vocab_size\r\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "\r\n",
    "def get_params():\r\n",
    "    def _one(shape):\r\n",
    "        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32)\r\n",
    "        return torch.nn.Parameter(ts, requires_grad=True)\r\n",
    "\r\n",
    "    # 隐藏层参数\r\n",
    "    W_xh = _one((num_inputs, num_hiddens))\r\n",
    "    W_hh = _one((num_hiddens,num_hiddens))\r\n",
    "    b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device, requires_grad=True))\r\n",
    "    # 输出层参数\r\n",
    "    W_hq = _one((num_hiddens, num_outputs))\r\n",
    "    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, requires_grad=True))\r\n",
    "    # 附上梯度\r\n",
    "    return nn.ParameterList([W_xh, W_hh, b_h, W_hq, b_q])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 定义模型\r\n",
    "\r\n",
    "我们根据循环神经网络的计算表达式实现该模型。首先定义init_rnn_state函数来返回初始化的隐藏状态。它返回由一个形状为(批量大小, 隐藏单元个数)的值为0的Tensor组成的元组。使用元组是为了更便于处理隐藏状态含有多个Tensor的情况。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "def init_rnn_state(batch_size, num_hiddens, device):\r\n",
    "    return(torch.zeros(batch_size, num_hiddens).to(device), )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面的rnn函数定义了在一个时间步里如何计算隐藏状态和输出。这里的激活函数使用了tanh函数。“多层感知机”一节中介绍过，当元素在实数域上均匀分布时，tanh函数值的均值为0。\r\n",
    "\r\n",
    "![](./img/rnn.svg)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "def rnn(inputs, state, params):\r\n",
    "#  inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵\r\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\r\n",
    "    H, = state\r\n",
    "    outputs = []\r\n",
    "    for X in inputs:\r\n",
    "        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)\r\n",
    "        Y = torch.matmul(H, W_hq) + b_q\r\n",
    "        outputs.append(Y)\r\n",
    "    return outputs, (H,)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "# a test\r\n",
    "\r\n",
    "state  = init_rnn_state(X.shape[0], num_hiddens, device)\r\n",
    "inputs = to_onehot(X.to(device), vocab_size, device)\r\n",
    "params = get_params()\r\n",
    "outputs,state_new = rnn(inputs, state, params)\r\n",
    "\r\n",
    "len(outputs), outputs[0].shape, state_new[0].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5, torch.Size([2, 1027]), torch.Size([2, 256]))"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 定义预测函数\r\n",
    "以下函数基于前缀prefix（含有数个字符的字符串）来预测接下来的num_chars个字符。这个函数稍显复杂，其中我们将循环神经单元rnn设置成了函数参数，这样在后面小节介绍其他循环神经网络时能重复使用这个函数。\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state, num_hiddens,vocab_size, device, idx_to_char, char_to_idx):\r\n",
    "    state = init_rnn_state(1, num_hiddens, device)\r\n",
    "    output = [char_to_idx[prefix[0]]]\r\n",
    "    for t in range(num_chars + len(prefix) - 1):\r\n",
    "        # 将上一时间片的输出作为当前时间步的输入\r\n",
    "        X = to_onehot(torch.tensor([[output[-1]]], device = device), vocab_size)\r\n",
    "        # print(X[0].size())\r\n",
    "        # 计算输出和隐藏状态\r\n",
    "        (Y, state) = rnn(X, state, params)\r\n",
    "        # 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符\r\n",
    "        if t < len(prefix) - 1:\r\n",
    "            output.append(char_to_idx[prefix[t+1]])\r\n",
    "        else:\r\n",
    "            output.append(int(Y[0].argmax(dim = 1).item()))\r\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们先测试一下predict_rnn函数。我们将根据前缀“分开”创作长度为10个字符（不考虑前缀长度）的一段歌词。因为模型参数为随机值，所以预测结果也是随机的。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "predict_rnn('分开', 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,device, idx_to_char, char_to_idx)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'分开缘书惯胡纳叫奇呆可提'"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 裁剪梯度\r\n",
    "\r\n",
    "循环神经网络中较容易出现梯度衰减或梯度爆炸。我们会在“通过时间反向传播”一节中解释原因。为了应对梯度爆炸，我们可以裁剪梯度（clip gradient）。假设我们把所有模型参数梯度的元素拼接成一个向量 $\\boldsymbol{g}$，并设裁剪的阈值是$\\theta$。裁剪后的梯度\r\n",
    "\r\n",
    "$$ \\min\\left(\\frac{\\theta}{\\|\\boldsymbol{g}\\|}, 1\\right)\\boldsymbol{g}$$\r\n",
    "的$L_2$范数不超过$\\theta$。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "def grad_clipping(params, theta, device):\r\n",
    "    norm = torch.Tensor([0]).to(device)\r\n",
    "    for param in params:\r\n",
    "        norm += (param.grad.data ** 2).sum()\r\n",
    "\r\n",
    "    norm = norm.sqrt().item()\r\n",
    "    if  theta < norm:\r\n",
    "        for param in params:\r\n",
    "            param.grad.data.mul_(theta/ norm)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 困惑度\r\n",
    "\r\n",
    "我们通常使用困惑度（perplexity）来评价语言模型的好坏。回忆一下“softmax回归”一节中交叉熵损失函数的定义。困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，\r\n",
    "\r\n",
    "最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；\r\n",
    "最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；\r\n",
    "基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。\r\n",
    "显然，任何一个有效模型的困惑度必须小于类别个数。在本例中，困惑度必须小于词典大小vocab_size。\r\n",
    "\r\n",
    "### 定义模型训练参数\r\n",
    "\r\n",
    "定义模型训练函数\r\n",
    "跟之前章节的模型训练函数相比，这里的模型训练函数有以下几点不同：\r\n",
    "\r\n",
    "- 使用困惑度评价模型。\r\n",
    "- 在迭代模型参数前裁剪梯度。\r\n",
    "- 对时序数据采用不同采样方法将导致隐藏状态初始化的不同。相关讨论可参考“语言模型数据集（周杰伦专辑歌词）”一节。\r\n",
    "\r\n",
    "另外，考虑到后面将介绍的其他循环神经网络，为了更通用，这里的函数实现更长一些。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\r\n",
    "                          vocab_size, device, corpus_indices, idx_to_char,\r\n",
    "                          char_to_idx, is_random_iter, num_epochs, num_steps,\r\n",
    "                          lr, clipping_theta, batch_size, pred_period,\r\n",
    "                          pred_len, prefixes):\r\n",
    "    if is_random_iter:\r\n",
    "        data_iter_fn = d2l.data_iter_random\r\n",
    "    else:\r\n",
    "        data_iter_fn = d2l.data_iter_consecutive\r\n",
    "    params = get_params()\r\n",
    "    loss = nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "    for epoch in range(num_epochs):\r\n",
    "        #  如使用相邻采样，在epoch开始时初始化隐藏状态\r\n",
    "        if not is_random_iter: \r\n",
    "            state = init_rnn_state(batch_size, num_hiddens, device)\r\n",
    "        \r\n",
    "        l_sum, n, start = 0.0, 0, time.time()\r\n",
    "        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)\r\n",
    "        for X, Y in data_iter:\r\n",
    "            if is_random_iter:  # 如使用随机采样，在每个小批量更新前初始化隐藏状态\r\n",
    "                state = init_rnn_state(batch_size, num_hiddens, device)\r\n",
    "            else:  # 否则需要使用detach函数从计算图分离隐藏状态\r\n",
    "                for s in state:\r\n",
    "                    s.detach_()\r\n",
    "\r\n",
    "            inputs = to_onehot(X, vocab_size, device)\r\n",
    "            # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵\r\n",
    "            (outputs, state) = rnn(inputs, state, params)\r\n",
    "            # 拼接之后形状为(num_steps * batch_size, vocab_size)\r\n",
    "            outputs = torch.cat(outputs, dim=0)\r\n",
    "            # Y的形状是(batch_size, num_steps)，转置后再变成长度为\r\n",
    "            # batch_size * num_steps 的向量，这样跟输出的行一一对应\r\n",
    "            y = torch.transpose(Y, 0, 1).contiguous().view(-1).long()\r\n",
    "            \r\n",
    "\r\n",
    "            # 使用交叉熵损失计算平均分类误差\r\n",
    "            l = loss(outputs, y).mean()\r\n",
    "\r\n",
    "            # 梯度清0\r\n",
    "            if params[0].grad is not None:\r\n",
    "                for param in params:\r\n",
    "                    param.grad.data.zero_()\r\n",
    "\r\n",
    "            l.backward()\r\n",
    "            grad_clipping(params, clipping_theta, device)  # 裁剪梯度\r\n",
    "            d2l.sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均\r\n",
    "            l_sum += l.data.item() * y.shape[0] # 把平均乘回来\r\n",
    "            n += y.shape[0]\r\n",
    "\r\n",
    "        if (epoch + 1) % pred_period == 0:\r\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' %\r\n",
    "                  (epoch + 1, math.exp(l_sum / n), time.time() - start))\r\n",
    "            for prefix in prefixes:\r\n",
    "                print(\r\n",
    "                    ' -',\r\n",
    "                    predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,\r\n",
    "                                num_hiddens, vocab_size, device, idx_to_char,\r\n",
    "                                char_to_idx))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 训练模型并创作歌词\r\n",
    "\r\n",
    "首先，设置模型超参数。我们将根据前缀“分开”和“不分开”分别创作长度为50个字符（不考虑前缀长度）的一段歌词。我们每过50个迭代周期便根据当前训练的模型创作一段歌词。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "num_epochs, num_steps, batch_size, lr, clipping_theta = 250, 35, 32, 1e2, 1e-2\r\n",
    "pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']\r\n",
    "\r\n",
    "# 下面采用随机采样训练模型并创作歌词。\r\n",
    "train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\r\n",
    "                      vocab_size, device, corpus_indices, idx_to_char,\r\n",
    "                      char_to_idx, True, num_epochs, num_steps, lr,\r\n",
    "                      clipping_theta, batch_size, pred_period, pred_len,\r\n",
    "                      prefixes)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-6ae0545dc361>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 下面采用随机采样训练模型并创作歌词。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n\u001b[0m\u001b[0;32m      6\u001b[0m                       \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_to_char\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                       \u001b[0mchar_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-d19333c86ccc>\u001b[0m in \u001b[0;36mtrain_and_predict_rnn\u001b[1;34m(rnn, get_params, init_rnn_state, num_hiddens, vocab_size, device, corpus_indices, idx_to_char, char_to_idx, is_random_iter, num_epochs, num_steps, lr, clipping_theta, batch_size, pred_period, pred_len, prefixes)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mgrad_clipping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclipping_theta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 裁剪梯度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0md2l\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 因为误差已经取过均值，梯度不用再做平均\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m             \u001b[0ml_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# 把平均乘回来\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mn\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\admin\\Desktop\\面向蜂群协同的多智能体强化学习研究\\MADRL_code\\learning_pytorch\\动手学深度学习PYTORCH版\\d2lzh\\utils.py\u001b[0m in \u001b[0;36msgd\u001b[1;34m(params, lr, batch_size)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;31m# 沿batch维求了平均了。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;31m# 注意这里更改param时用的param.data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 小结\r\n",
    "\r\n",
    "- 可以用基于字符级循环神经网络的语言模型来生成文本序列，例如创作歌词。\r\n",
    "- 当训练循环神经网络时，为了应对梯度爆炸，可以裁剪梯度。\r\n",
    "- 困惑度是对交叉熵损失函数做指数运算后得到的值。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 循环神经网络的简洁实现\r\n",
    "\r\n",
    "即使用nn来实现"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# 读取周杰伦歌词Datasets\r\n",
    "\r\n",
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "# 定义model\r\n",
    "\r\n",
    "num_hiddens = 256\r\n",
    "rnn_layer = nn.RNN(input_size = vocab_size, hidden_size = num_hiddens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "与上⼀节中实现的循环神经⽹络不同，这⾥ rnn_layer 的输⼊形状为(时间步数, 批量⼤⼩, 输⼊个\r\n",
    "数)。其中输⼊个数即one-hot向量⻓度（词典⼤⼩）。此外， rnn_layer 作为 nn.RNN 实例，在前向\r\n",
    "计算后会分别返回输出和隐藏状态h，其中输出指的是隐藏层在各个时间步上计算并输出的隐藏状态，\r\n",
    "它们通常作为后续输出层的输⼊。需要强调的是，该“输出”本身并不涉及输出层计算，形状为(时间步\r\n",
    "数, 批量⼤⼩, 隐藏单元个数)。⽽ nn.RNN 实例在前向计算返回的隐藏状态指的是隐藏层在最后时间步\r\n",
    "的隐藏状态：当隐藏层有多层时，每⼀层的隐藏状态都会记录在该变量中；对于像⻓短期记忆\r\n",
    "（LSTM），隐藏状态是⼀个元组(h, c)，即hidden state和cell state。我们会在本章的后⾯介绍⻓短期\r\n",
    "记忆和深度循环神经⽹络。 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "num_steps = 35\r\n",
    "batch_size = 2\r\n",
    "state = None\r\n",
    "X = torch.rand(num_steps, batch_size, vocab_size)\r\n",
    "Y, state_new = rnn_layer(X, state)\r\n",
    "print(state_new.shape)\r\n",
    "print(Y.shape, len(state_new), state_new[0].shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 2, 256])\n",
      "torch.Size([35, 2, 256]) 1 torch.Size([2, 256])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来我们继承 Module 类来定义⼀个完整的循环神经⽹络。它⾸先将输⼊数据使⽤one-hot向量表示\r\n",
    "后输⼊到 rnn_layer 中，然后使⽤全连接输出层得到输出。输出个数等于词典⼤⼩ vocab_size 。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "class RNNModel(nn.Module):\r\n",
    "    def __init__(self,rnn_layer, vocab_size):\r\n",
    "        super(RNNModel, self).__init__()\r\n",
    "        self.rnn = rnn_layer\r\n",
    "        self.hidden_size = rnn_layer.hidden_size * (2 if rnn_layer.bidirectional else 1)\r\n",
    "        self.vocab_size = vocab_size\r\n",
    "        self.dense  = nn.Linear(self.hidden_size, vocab_size)\r\n",
    "        self.state = None\r\n",
    "\r\n",
    "    def forward(self, inputs, state):\r\n",
    "        # 获取one-hot向量表示\r\n",
    "        X = d2l.to_onehot(inputs, self.vocab_size) # X is a list\r\n",
    "        Y, self.state = self.rnn(torch.stack(X), state)\r\n",
    "        # 全连接层会首先将Y的形状变成（num_steps * batch_size, num_hiddens),\r\n",
    "        # 它的输出形状为（num_steps,* batch_size, vocab_size)\r\n",
    "        output = self.dense(Y.view(-1, Y.shape[-1]))\r\n",
    "        return output, self.state"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "# train model\r\n",
    "\r\n",
    "# 下⾯定义⼀个预测函数。这⾥的实现区别在于前向计算和初始化隐藏状态的函数接⼝\r\n",
    "\r\n",
    "def predict_rnn_pytorch(prefix, num_chars,model, vocab_size, device, idx_to_char,char_to_idx):\r\n",
    "    state = None\r\n",
    "    # output会记录prefix + 输出\r\n",
    "    output = [char_to_idx[prefix[0]]] \r\n",
    "    for t in range(num_chars + len(prefix) - 1):\r\n",
    "        X = torch.tensor([output[-1]], device=device).view(1,1)\r\n",
    "        if state is not None:\r\n",
    "            if isinstance(state, tuple): # LSTM, state:(h,c)\r\n",
    "                state = (state[0].to(device), state[1].to(device))\r\n",
    "            else:\r\n",
    "                state = state.to(device)\r\n",
    "\r\n",
    "        (Y, state) = model(X,state)\r\n",
    "        if t < len(prefix) -1:\r\n",
    "            output.append(char_to_idx[prefix[t + 1]])\r\n",
    "        else:\r\n",
    "            output.append(int(Y.argmax(dim = 1).item()))\r\n",
    "\r\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "source": [
    "# 使用权重为随机值的模型来预测一次\r\n",
    "\r\n",
    "model = RNNModel(rnn_layer, vocab_size).to(device)\r\n",
    "predict_rnn_pytorch('分开', 10, model, vocab_size, device, idx_to_char, char_to_idx)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'分开弄彿饿灵灵灵灵灵灵灵'"
      ]
     },
     "metadata": {},
     "execution_count": 165
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "source": [
    "def train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\r\n",
    "                                corpus_indices, idx_to_char, char_to_idx,\r\n",
    "                                num_epochs, num_steps, lr, clipping_theta,\r\n",
    "                                batch_size, pred_period, pred_len, prefixes):\r\n",
    "    loss = nn.CrossEntropyLoss()\r\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n",
    "    model.to(device)\r\n",
    "    state = None\r\n",
    "    for epoch in range(num_epochs):\r\n",
    "        l_sum, n, start = 0.0, 0, time.time()\r\n",
    "        data_iter = d2l.data_iter_consecutive(corpus_indices, batch_size, num_steps, device) # 相邻采样\r\n",
    "        for X, Y in data_iter:\r\n",
    "            if state is not None:\r\n",
    "                # 使用detach函数从计算图分离隐藏状态, 这是为了\r\n",
    "                # 使模型参数的梯度计算只依赖一次迭代读取的小批量序列(防止梯度计算开销太大)\r\n",
    "                if isinstance (state, tuple): # LSTM, state:(h, c)  \r\n",
    "                    state = (state[0].detach(), state[1].detach())\r\n",
    "                else:   \r\n",
    "                    state = state.detach()\r\n",
    "    \r\n",
    "            (output, state) = model(X, state) # output: 形状为(num_steps * batch_size, vocab_size)\r\n",
    "            \r\n",
    "            # Y的形状是(batch_size, num_steps)，转置后再变成长度为\r\n",
    "            # batch * num_steps 的向量，这样跟输出的行一一对应\r\n",
    "            y = torch.transpose(Y, 0, 1).contiguous().view(-1)\r\n",
    "            l = loss(output, y.long())\r\n",
    "            \r\n",
    "            optimizer.zero_grad()\r\n",
    "            l.backward()\r\n",
    "            # 梯度裁剪\r\n",
    "            d2l.grad_clipping(model.parameters(), clipping_theta, device)\r\n",
    "            optimizer.step()\r\n",
    "            l_sum += l.item() * y.shape[0]\r\n",
    "            n += y.shape[0]\r\n",
    "        \r\n",
    "        try:\r\n",
    "            perplexity = math.exp(l_sum / n)\r\n",
    "        except OverflowError:\r\n",
    "            perplexity = float('inf')\r\n",
    "        if (epoch + 1) % pred_period == 0:\r\n",
    "            print('epoch %d, perplexity %f, time %.2f sec' % (\r\n",
    "                epoch + 1, perplexity, time.time() - start))\r\n",
    "            for prefix in prefixes:\r\n",
    "                print(' -', predict_rnn_pytorch(\r\n",
    "                    prefix, pred_len, model, vocab_size, device, idx_to_char,\r\n",
    "                    char_to_idx))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "source": [
    "num_epochs, batch_size, lr, clipping_theta = 250, 32, 1e-3, 1e-2 # 注意这里的学习率设置\r\n",
    "pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']\r\n",
    "train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\r\n",
    "                            corpus_indices, idx_to_char, char_to_idx,\r\n",
    "                            num_epochs, num_steps, lr, clipping_theta,\r\n",
    "                            batch_size, pred_period, pred_len, prefixes)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 50, perplexity 8.185968, time 4.67 sec\n",
      " - 分开始在美 不起 你已经离开我 别知道  你说了这样打我妈  想你你的微笑每天都能不想  不知不觉 你不\n",
      " - 不分开 我想你这样打我妈  想要你的微笑每天的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人\n",
      "epoch 100, perplexity 1.219531, time 2.31 sec\n",
      " - 分开 一片人老日  后悔着对不起 干什么 干什么 我不开任督二脉 干什么 干什么 东亚病夫的招牌 干什么\n",
      " - 不分开 我后知后觉 我想一定实我 一样了口伤人 不知道  没有了每天每天都想想想想你想著你 这样打甜蜜 让\n",
      "epoch 150, perplexity 1.058199, time 2.61 sec\n",
      " - 分开 在色了口被 怎么都平  都没有 我一定有痛 我妈妈汉 你我的看爸你  我去 这里很听暴力因 我有那\n",
      " - 不分开 了路怎么小   后再后离我想三回忆 着对一起被颗的黑色幽默 没有了永远 这里的美演出的的爸要 有一\n",
      "epoch 200, perplexity 1.034115, time 1.65 sec\n",
      " - 分开 一句人老斑  像不上的一场悲剧 我可以让我满就这样牵着你的手不放开 爱能不能够永远单纯没有悲哀 我\n",
      " - 不分开 了又怎么小 有不能 又过了最后再想了 你在我 三你   想你在你爸我  想去 败给你的黑色幽默 说\n",
      "epoch 250, perplexity 1.021452, time 1.76 sec\n",
      " - 分开 一句人老斑鸠 像不上的语怪我 印地安的传说的印地安的爱像的 我 是话不话 我迷蒙蒙 你怎么我 说我\n",
      " - 不分开 了路怎么找 我不着我 手有一阵莫名感动 我想带你 回我的外婆家 一起看着日落 一直到我们都睡着 我\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 小结\r\n",
    " \r\n",
    "- torch的nn.RNN实例在前向计算后会分别返回输出和隐藏状态。该前向计算并不涉及输出层计算。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 通过时间反向传播\r\n",
    "\r\n",
    "如果不裁剪梯度进行训练，就会发现模型无法正常训练。为了深刻理解这一现象，本节将介绍循环神经网络中梯度的计算和存储方法，即通过时间反向传播（back-propagation through time）。\r\n",
    "\r\n",
    "我们在“正向传播、反向传播和计算图”一节中介绍了神经网络中梯度计算与存储的一般思路，并强调正向传播和反向传播相互依赖。正向传播在循环神经网络中比较直观，而通过时间反向传播其实是反向传播在循环神经网络中的具体应用。我们需要将循环神经网络按时间步展开，从而得到模型变量和参数之间的依赖关系，并依据链式法则应用反向传播计算并存储梯度。\r\n",
    "\r\n",
    "时间步数为T 的损失函数 L 定义为\r\n",
    "$$ L = \\frac{1}{T} \\sum_{t=1}^T \\ell (\\boldsymbol{o}_t, y_t). $$\r\n",
    "\r\n",
    "### ⼩结\r\n",
    "- 通过时间反向传播是反向传播在循环神经⽹络中的具体应⽤。\r\n",
    "- 当总的时间步数较⼤或者当前时间步较⼩时，循环神经⽹络的梯度较容易出现衰减或爆炸"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "07efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}