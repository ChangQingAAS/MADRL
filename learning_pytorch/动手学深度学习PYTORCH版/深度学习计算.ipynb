{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "%matplotlib inline\r\n",
    "import random\r\n",
    "import time \r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from torch.nn import init\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from IPython import display\r\n",
    "from collections import OrderedDict\r\n",
    "import sys\r\n",
    "import d2lzh as d2l\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "from mpl_toolkits import mplot3d\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 模型构造\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1.1 继承 MODULE 类来构造模型\r\n",
    "\r\n",
    "Module 类是 nn 模块⾥提供的⼀个模型构造类，是所有神经⽹络模块的基类，我们可以继承它来定义\r\n",
    "我们想要的模型。下⾯继承 Module 类构造本节开头提到的多层感知机。这⾥定义的 MLP 类᯿载了\r\n",
    "Module 类的 __init__ 函数和 forward 函数。它们分别⽤于创建模型参数和定义前向计算。前向计\r\n",
    "算也即正向传播。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "import torch\r\n",
    "from torch import nn\r\n",
    "\r\n",
    "#  MLP`类中无须定义反向传播函数。\r\n",
    "#  系统将通过自动求梯度而自动生成反向传播所需的`backward`函数。\r\n",
    "\r\n",
    "class MLP(nn.Module):\r\n",
    "    # 声明带有模型参数的层，这里声明了两个全连接层\r\n",
    "    def __init__(self, **kwargs):\r\n",
    "        # 调用MLP父类Module的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数\r\n",
    "        super(MLP, self).__init__(**kwargs)\r\n",
    "        self.hidden = nn.Linear(784, 256) # 隐藏层\r\n",
    "        self.act = nn.ReLU()\r\n",
    "        self.output = nn.Linear(256, 10)  # 输出层\r\n",
    "        \r\n",
    "    # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出\r\n",
    "    def forward(self, x):\r\n",
    "        a = self.act(self.hidden(x))\r\n",
    "        return self.output(a)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "# 实例化MLP类得到模型变量net\r\n",
    "X = torch.rand(2,784)\r\n",
    "net = MLP()\r\n",
    "print(net)\r\n",
    "net(X).shape # 2 10\r\n",
    "net(X)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.0381,  0.3193,  0.2054,  0.0360, -0.0974, -0.0298, -0.0461,  0.0044,\n",
       "          0.0503, -0.1548],\n",
       "        [ 0.0811,  0.2190,  0.1913,  0.0872, -0.0776,  0.0412, -0.2329,  0.0557,\n",
       "          0.0362, -0.2745]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "注意，这里并没有将`Module`类命名为`Layer`（层）或者`Model`（模型）之类的名字，这是因为该类是一个可供自由组建的部件。它的子类既可以是一个层（如PyTorch提供的`Linear`类），又可以是一个模型（如这里定义的`MLP`类），或者是模型的一个部分。我们下面通过两个例子来展示它的灵活性。\r\n",
    "\r\n",
    "## 4.1.2 `Module`的子类\r\n",
    "PyTorch还实现了继承自`Module`的可以方便构建模型的类: 如`Sequential`、`ModuleList`和`ModuleDict`等等。\r\n",
    "\r\n",
    "### 4.1.2.1 `Sequential`类\r\n",
    "当模型的前向计算为简单串联各个层的计算时，`Sequential`类可以通过更加简单的方式定义模型。这正是`Sequential`类的目的：它可以接收一个子模块的有序字典（OrderedDict）或者一系列子模块作为参数来逐一添加`Module`的实例，而模型的前向计算就是将这些实例按添加的顺序逐一计算。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "# 实现一个与`Sequential`类有相同功能的`MySequential`类\r\n",
    "\r\n",
    "class MySequential(nn.Module):\r\n",
    "    def __init__(self, *args):\r\n",
    "        super(MySequential, self).__init__()\r\n",
    "        if len(args) == 1 and isinstance(args[0], OrderedDict): # 如果传入的是一个OrderedDict\r\n",
    "            for key, module in args[0].items():\r\n",
    "                self.add_module(key, module)  # add_module方法会将module添加进self._modules(一个OrderedDict)\r\n",
    "        else:  # 传入的是一些Module\r\n",
    "            for idx, module in enumerate(args):\r\n",
    "                self.add_module(str(idx), module)\r\n",
    "    def forward(self, input):\r\n",
    "        # self._modules返回一个 OrderedDict，保证会按照成员添加时的顺序遍历成员\r\n",
    "        for module in self._modules.values():\r\n",
    "            input = module(input)\r\n",
    "        return input\r\n",
    "\r\n",
    "# 用`MySequential`类来实现前面描述的`MLP`类，\r\n",
    "# 并使用随机初始化的模型做一次前向计算。\r\n",
    "X = torch.rand(2,784)\r\n",
    "net = MySequential(\r\n",
    "        nn.Linear(784, 256),\r\n",
    "        nn.ReLU(),\r\n",
    "        nn.Linear(256, 10),\r\n",
    "        )\r\n",
    "print(net)\r\n",
    "net(X)\r\n",
    "\r\n",
    "# 可以观察到这里`MySequential`类的使用跟3.10节（多层感知机的简洁实现）中`Sequential`类的使用没什么区别。"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MySequential(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.1853,  0.1869, -0.0744,  0.0117, -0.2149, -0.1122,  0.0158, -0.0172,\n",
       "         -0.1010, -0.1256],\n",
       "        [ 0.2070,  0.1760, -0.0286, -0.0443, -0.1740, -0.1356,  0.1430,  0.0134,\n",
       "         -0.0137, -0.1042]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.1.2.2 `ModuleList`类\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "# `ModuleList`接收一个子模块的列表作为输入，然后也可以类似List那样进行append和extend操作:\r\n",
    "\r\n",
    "net = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])\r\n",
    "net.append(nn.Linear(256, 10)) # # 类似List的append操作\r\n",
    "print(net[-1])  # 类似List的索引访问\r\n",
    "print(net)\r\n",
    "# net(torch.zeros(1, 784)) # 会报NotImplementedError"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear(in_features=256, out_features=10, bias=True)\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "# 4.1.2.3 ModuleDict 类\r\n",
    "# ModuleDict 接收⼀个⼦模块的字典作为输⼊, 然后也可以类似字典那样进⾏添加访问操作:\r\n",
    "\r\n",
    "net = nn.ModuleDict({\r\n",
    "    'linear': nn.Linear(784, 256),\r\n",
    "    'act': nn.ReLU(),\r\n",
    "})\r\n",
    "net['output'] = nn.Linear(256, 10) # 添加\r\n",
    "print(net['linear']) # 访问\r\n",
    "print(net.output)\r\n",
    "print(net)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear(in_features=784, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=10, bias=True)\n",
      "ModuleDict(\n",
      "  (linear): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "# 4.1.3 构造复杂的模型\r\n",
    "# 通过 get_constant 函数创建训练中不被迭代的参数，即常数参数\r\n",
    "\r\n",
    "class FancyMLP(nn.Module):\r\n",
    "    def __init__(self, **kwargs):\r\n",
    "        super(FancyMLP, self).__init__(**kwargs)\r\n",
    "\r\n",
    "        # 注意参数权重rand_weight（不是可训练模型参数\r\n",
    "        self.rand_weight = torch.rand((20,20), requires_grad=True)\r\n",
    "        self.linear = nn.Linear(20,20)\r\n",
    "\r\n",
    "    def forward(self,x):\r\n",
    "        x = self.linear(x)\r\n",
    "        # 使⽤创建的常数参数，以及nn.functional中的relu函数和mm函数\r\n",
    "        x = nn.functional.relu(torch.mm(x, self.rand_weight.data) + 1)\r\n",
    "\r\n",
    "        # 复用全连接层，等价于两个全连接层共享参数\r\n",
    "        x = self.linear(x)\r\n",
    "        # 控制流，这里我们需要调用item函数来返回标量进行比较\r\n",
    "        while x.norm().item() > 1:\r\n",
    "            x /= 2\r\n",
    "        if x.norm().item() < 0.8:\r\n",
    "            x *= 10\r\n",
    "        return x.sum()\r\n",
    "\r\n",
    "X = torch.rand(2, 20)\r\n",
    "net = FancyMLP()\r\n",
    "print(net)\r\n",
    "net(X)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FancyMLP(\n",
      "  (linear): Linear(in_features=20, out_features=20, bias=True)\n",
      ")\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(-0.6186, grad_fn=<SumBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "# 因为 FancyMLP 和 Sequential 类都是 Module 类的⼦类，所以我们可以嵌套调⽤它们\r\n",
    "\r\n",
    "class NestMLP(nn.Module):\r\n",
    "    def __init__(self, **kwargs):\r\n",
    "        super(NestMLP, self).__init__(**kwargs)\r\n",
    "        self.net = nn.Sequential(nn.Linear(40, 30), nn.ReLU())\r\n",
    "    def forward(self, x):\r\n",
    "        return self.net(x)\r\n",
    "\r\n",
    "net = nn.Sequential(NestMLP(), nn.Linear(30, 20), FancyMLP())\r\n",
    "\r\n",
    "X = torch.rand(2, 40)\r\n",
    "print(net)\r\n",
    "net(X)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n",
      "  (0): NestMLP(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=40, out_features=30, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=30, out_features=20, bias=True)\n",
      "  (2): FancyMLP(\n",
      "    (linear): Linear(in_features=20, out_features=20, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(16.8084, grad_fn=<SumBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 小结\r\n",
    "\r\n",
    "* 可以通过继承`Module`类来构造模型。\r\n",
    "* `Sequential`、`ModuleList`、`ModuleDict`类都继承自`Module`类。\r\n",
    "* 与`Sequential`不同，`ModuleList`和`ModuleDict`并没有定义一个完整的网络，它们只是将不同的模块存放在一起，需要自己定义`forward`函数。\r\n",
    "* 虽然`Sequential`等类可以使模型构造更加简单，但直接继承`Module`类可以极大地拓展模型构造的灵活性。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 模型参数的访问、初始化和共享\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "print(type(net.named_parameters()))\r\n",
    "for name, param in net.named_parameters():\r\n",
    "    print(name, param.size())\r\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'generator'>\n",
      "0.net.0.weight torch.Size([30, 40])\n",
      "0.net.0.bias torch.Size([30])\n",
      "1.weight torch.Size([20, 30])\n",
      "1.bias torch.Size([20])\n",
      "2.linear.weight torch.Size([20, 20])\n",
      "2.linear.bias torch.Size([20])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "可⻅返回的名字⾃动加上了层数的索引作为前缀。 我们再来访问 net 中单层的参数。对于使\r\n",
    "⽤ Sequential 类构造的神经⽹络，我们可以通过⽅括号 [] 来访问⽹络的任⼀层。索引0表示隐藏层\r\n",
    "为 Sequential 实例最先添加的层"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "for name, param in net[0].named_parameters():\r\n",
    "    print(name,param.size(),type(param))\r\n",
    "# 返回的 param 的 类 型为 torch.nn.parameter.Parameter ，\r\n",
    "# 其实这是 Tensor 的⼦类，\r\n",
    "# 和 Tensor 不同的是如果⼀个 Tensor 是 Parameter ，那么它会⾃动被添加到模型的参数列表⾥"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "net.0.weight torch.Size([30, 40]) <class 'torch.nn.parameter.Parameter'>\n",
      "net.0.bias torch.Size([30]) <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "class MyModel(nn.Module):\r\n",
    "    def __init__(self, **kwargs):\r\n",
    "        super(MyModel, self).__init__(**kwargs)\r\n",
    "        self.weight1 = nn.Parameter(torch.rand(20, 20))\r\n",
    "        self.weight2 = torch.rand(20, 20)\r\n",
    "    def forward(self, x):\r\n",
    "        pass\r\n",
    "\r\n",
    "n = MyModel()\r\n",
    "for name, param in n.named_parameters():\r\n",
    "    print(name)\r\n",
    "\r\n",
    "# 上⾯的代码中 weight1 在参数列表中但是 weight2 却没在参数列表中。"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "weight1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "# 因为 Parameter 是 Tensor ，即 Tensor 拥有的属性它都有，⽐如可以根据 data 来访问参数数值，⽤\r\n",
    "#  grad 来访问参数梯度。\r\n",
    "\r\n",
    "net = nn.Sequential(nn.Linear(4, 3), nn.ReLU(), nn.Linear(3, 1)) #pytorch已进⾏默认初始化\r\n",
    "print(net)\r\n",
    "X = torch.rand(2, 4)\r\n",
    "Y = net(X).sum()\r\n",
    "\r\n",
    "weight_0 = list(net[0].parameters())[0]\r\n",
    "print(weight_0.data)\r\n",
    "print(weight_0.grad) # 反向传播前梯度为None\r\n",
    "Y.backward()\r\n",
    "print(weight_0.grad)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "tensor([[ 0.2485,  0.0379,  0.4240,  0.4590],\n",
      "        [-0.0016,  0.2447,  0.1557,  0.0214],\n",
      "        [ 0.1231,  0.3365,  0.2719,  0.4871]])\n",
      "None\n",
      "tensor([[ 0.1329,  0.2923,  0.3430,  0.4098],\n",
      "        [-0.2323, -0.5109, -0.5995, -0.7163],\n",
      "        [-0.2829, -0.6222, -0.7302, -0.8724]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2.2 初始化模型参数\r\n",
    "PyTorch的 init 模块⾥提供了多种预设的初始化⽅法"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "for name, param in net.named_parameters():\r\n",
    "    if 'weight' in name:\r\n",
    "        init.normal_(param, mean=0, std=0.01)\r\n",
    "        print(name, param.data)\r\n",
    "\r\n",
    "for name, param in net.named_parameters():\r\n",
    "    if 'bias' in name:\r\n",
    "        init.constant_(param, val=0)\r\n",
    "        print(name, param.data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.weight tensor([[ 0.0036, -0.0054,  0.0016, -0.0094],\n",
      "        [ 0.0085, -0.0072, -0.0040, -0.0098],\n",
      "        [ 0.0102, -0.0034,  0.0138,  0.0017]])\n",
      "2.weight tensor([[ 0.0082,  0.0095, -0.0060]])\n",
      "0.bias tensor([0., 0., 0.])\n",
      "2.bias tensor([0.])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2.3 ⾃定义初始化⽅法"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "先来看看PyTorch是怎么实现这些初始化⽅法的，例\r\n",
    "如 torch.nn.init.normal_ ：\r\n",
    "``` python\r\n",
    "def normal_(tensor, mean=0, std=1):\r\n",
    "    with torch.no_grad():\r\n",
    "        return tensor.normal_(mean, std)\r\n",
    "```\r\n",
    "可以看到这就是一个inplace改变`Tensor`值的函数，而且这个过程是不记录梯度的。\r\n",
    "类似的我们来实现一个自定义的初始化方法。在下面的例子里，我们令权重有一半概率初始化为0，有另一半概率初始化为$[-10,-5]$和$[5,10]$两个区间里均匀分布的随机数。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "def init_weight_(tensor):\r\n",
    "    with torch.no_grad():\r\n",
    "        tensor.uniform_(-10,10)\r\n",
    "        tensor *= (tensor.abs() >= 5).float()\r\n",
    "\r\n",
    "for name,param in net.named_parameters():\r\n",
    "    if 'weight' in name:\r\n",
    "        init_weight_(param)\r\n",
    "        print(name,param.data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.weight tensor([[ 0.0000,  0.0000, -0.0000,  0.0000],\n",
      "        [-8.4156, -8.7313, -9.4846, -7.4093],\n",
      "        [-0.0000,  8.5241,  7.9221,  0.0000]])\n",
      "2.weight tensor([[-7.8491,  0.0000, -0.0000]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "# 此外，参考2.3.2节，我们还可以通过改变这些参数的`data`来改写模型参数值同时不会影响梯度:\r\n",
    "\r\n",
    "for name, param in net.named_parameters():\r\n",
    "    if 'bias' in name:\r\n",
    "        param.data += 1\r\n",
    "        print(name, param.data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.bias tensor([1., 1., 1.])\n",
      "2.bias tensor([1.])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2.4 共享模型参数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "linear = nn.Linear(1, 1, bias=False)\r\n",
    "net = nn.Sequential(linear, linear)\r\n",
    "print(net)\r\n",
    "for name, param in net.named_parameters():\r\n",
    "    init.constant_(param, val=3)\r\n",
    "    print(name, param.data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=False)\n",
      "  (1): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n",
      "0.weight tensor([[3.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "# 在内存中，这两个线性层其实⼀个对象\r\n",
    "\r\n",
    "print(id(net[0]) == id(net[1]))\r\n",
    "print(id(net[0].weight) == id(net[1].weight))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "# 因为模型参数⾥包含了梯度，所以在反向传播计算时，这些共享的参数的梯度是累加的\r\n",
    "\r\n",
    "x = torch.ones(1, 1)\r\n",
    "y = net(x).sum()\r\n",
    "print(y)\r\n",
    "y.backward()\r\n",
    "print(net[0].weight.grad) # 单次梯度是3，两次所以就是6"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(9., grad_fn=<SumBackward0>)\n",
      "tensor([[6.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4 自定义层\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.4.1 不含模型参数的⾃定义层\r\n",
    "\r\n",
    "事实上，这和4.1节（模型构造）中介绍的使用`Module`类构造模型类似。\r\n",
    "\r\n",
    "下面的`CenteredLayer`类通过继承`Module`类自定义了一个将输入减掉均值后输出的层，并将层的计算定义在了`forward`函数里。这个层里不含模型参数。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "class CenteredLayer(nn.Module):\r\n",
    "    def __init__(self, **kwargs):\r\n",
    "        super(CenteredLayer, self).__init__(**kwargs)\r\n",
    "    def forward(self, x):\r\n",
    "        return x - x.mean()\r\n",
    "\r\n",
    "layer = CenteredLayer()\r\n",
    "layer(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float))\r\n",
    "\r\n",
    "net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())\r\n",
    "y = net(torch.rand(4, 8))\r\n",
    "y.mean().item()\r\n",
    "# 因为均值是浮点数，所以它的值是⼀个很接近0的数。"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-2.3283064365386963e-09"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.4.2 含模型参数的⾃定义层\r\n",
    "\r\n",
    "在4.2节（模型参数的访问、初始化和共享）中介绍了 Parameter 类其实是 Tensor 的⼦类，如果⼀\r\n",
    "个 Tensor 是 Parameter ，那么它会⾃动被添加到模型的参数列表⾥。所以在⾃定义含模型参数的层\r\n",
    "时，我们应该将参数定义成 Parameter ，除了像4.2.1节那样直接定义成 Parameter 类外，还可以使\r\n",
    "⽤ ParameterList 和 ParameterDict 分别定义参数的列表和字典。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ParameterList 接收⼀个 Parameter 实例的列表作为输⼊然后得到⼀个参数列表，使⽤的时候可以\r\n",
    "⽤索引来访问某个参数，另外也可以使⽤ append 和 extend 在列表后⾯新增参数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "class MyListDense(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(MyListDense, self).__init__()\r\n",
    "        self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)])\r\n",
    "        self.params.append(nn.Parameter(torch.randn(4, 1)))\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        for i in range(len(self.params)):\r\n",
    "            x = torch.mm(x, self.params[i])\r\n",
    "        return x\r\n",
    "net = MyListDense()\r\n",
    "print(net)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MyListDense(\n",
      "  (params): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (2): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (3): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "⽽ ParameterDict 接收⼀个 Parameter 实例的字典作为输⼊然后得到⼀个参数字典，然后可以按照\r\n",
    "字典的规则使⽤了。例如使⽤ update() 新增参数，使⽤ keys() 返回所有键值，使⽤ items() 返回\r\n",
    "所有键值对等等"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "class MyDictDense(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(MyDictDense, self).__init__()\r\n",
    "        self.params = nn.ParameterDict({\r\n",
    "                'linear1': nn.Parameter(torch.randn(4, 4)),\r\n",
    "                'linear2': nn.Parameter(torch.randn(4, 1))\r\n",
    "        })\r\n",
    "        self.params.update({'linear3': nn.Parameter(torch.randn(4, 2))}) # 新增\r\n",
    "\r\n",
    "    def forward(self, x, choice='linear1'):\r\n",
    "        return torch.mm(x, self.params[choice])\r\n",
    "\r\n",
    "net = MyDictDense()\r\n",
    "print(net)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MyDictDense(\n",
      "  (params): ParameterDict(\n",
      "      (linear1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (linear2): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "      (linear3): Parameter containing: [torch.FloatTensor of size 4x2]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "# 这样就可以根据传⼊的键值来进⾏不同的前向传播：\r\n",
    "x = torch.ones(1, 4)\r\n",
    "\r\n",
    "print(net(x, 'linear1'))\r\n",
    "print(net(x, 'linear2'))\r\n",
    "print(net(x, 'linear3'))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-3.3331,  1.1265, -1.1502,  1.1859]], grad_fn=<MmBackward>)\n",
      "tensor([[-0.1026]], grad_fn=<MmBackward>)\n",
      "tensor([[-2.1482, -4.6740]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "# 也可以使⽤⾃定义层构造模型\r\n",
    "\r\n",
    "net = nn.Sequential(\r\n",
    "    MyDictDense(),\r\n",
    "    MyListDense(),\r\n",
    ")\r\n",
    "print(net)\r\n",
    "print(net(x))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n",
      "  (0): MyDictDense(\n",
      "    (params): ParameterDict(\n",
      "        (linear1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (linear2): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "        (linear3): Parameter containing: [torch.FloatTensor of size 4x2]\n",
      "    )\n",
      "  )\n",
      "  (1): MyListDense(\n",
      "    (params): ParameterList(\n",
      "        (0): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (2): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (3): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "    )\n",
      "  )\n",
      ")\n",
      "tensor([[-11.4312]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.5 读取和存储\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "在实际中，我们\r\n",
    "有时需要把训练好的模型部署到很多不同的设备。在这种情况下，我们可以把内存中训练好的模型参数\r\n",
    "存储在硬盘上供后续读取使⽤"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "# 4.5.1 读写 TENSOR\r\n",
    "\r\n",
    "x = torch.ones(3)\r\n",
    "y = torch.zeros(4)\r\n",
    "torch.save([x, y], 'xy.pt')\r\n",
    "xy_list = torch.load('xy.pt')\r\n",
    "xy_list\r\n",
    "\r\n",
    "torch.save({'x': x, 'y': y}, 'xy_dict.pt')\r\n",
    "xy = torch.load('xy_dict.pt')\r\n",
    "xy"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'x': tensor([1., 1., 1.]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.5.2 读写模型\r\n",
    "在 PyTorch 中 ， Module 的 可 学 习 参 数 ( 即权᯿和偏差 ) ，模块模型包含在参数中 ( 通 过\r\n",
    "model.parameters() 访问)。 state_dict 是⼀个从参数名称隐射到参数 Tesnor 的字典对象。\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "class MLP(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(MLP, self).__init__()\r\n",
    "        self.hidden = nn.Linear(3, 2)\r\n",
    "        self.act = nn.ReLU()\r\n",
    "        self.output = nn.Linear(2, 1)\r\n",
    "    def forward(self, x):\r\n",
    "        a = self.act(self.hidden(x))\r\n",
    "        return self.output(a)\r\n",
    "        \r\n",
    "net = MLP()\r\n",
    "net.state_dict()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[-0.5658, -0.2694, -0.3134],\n",
       "                      [-0.5666,  0.3352,  0.0934]])),\n",
       "             ('hidden.bias', tensor([-0.0631,  0.0924])),\n",
       "             ('output.weight', tensor([[ 0.1711, -0.1740]])),\n",
       "             ('output.bias', tensor([-0.5386]))])"
      ]
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "注意，只有具有可学习参数的层(卷积层、线性层等)才有 state_dict 中的条⽬。\r\n",
    "\r\n",
    "优化器( optim )也有⼀个 state_dict ，其中包含关于优化器状态以及所使⽤的超参数的信息。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\r\n",
    "optimizer.state_dict()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'state': {},\n",
       " 'param_groups': [{'lr': 0.001,\n",
       "   'momentum': 0.9,\n",
       "   'dampening': 0,\n",
       "   'weight_decay': 0,\n",
       "   'nesterov': False,\n",
       "   'params': [0, 1, 2, 3]}]}"
      ]
     },
     "metadata": {},
     "execution_count": 116
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.5.2.2 保存和加载模型\r\n",
    "\r\n",
    "PyTorch中保存和加载训练模型有两种常见的方法:\r\n",
    "1. 仅保存和加载模型参数(`state_dict`)；\r\n",
    "2. 保存和加载整个模型。\r\n",
    "3. \r\n",
    "##### 1. 保存和加载`state_dict`(推荐方式)\r\n",
    "\r\n",
    "保存：\r\n",
    "``` python\r\n",
    "torch.save(model.state_dict(), PATH) # 推荐的文件后缀名是pt或pth\r\n",
    "```\r\n",
    "\r\n",
    "加载：\r\n",
    "``` python\r\n",
    "model = TheModelClass(*args, **kwargs)\r\n",
    "model.load_state_dict(torch.load(PATH))\r\n",
    "```\r\n",
    "\r\n",
    "##### 2. 保存和加载整个模型\r\n",
    "保存：\r\n",
    "``` python\r\n",
    "torch.save(model, PATH)\r\n",
    "```\r\n",
    "加载：\r\n",
    "``` python\r\n",
    "model = torch.load(PATH)\r\n",
    "```\r\n",
    "\r\n",
    "我们采用推荐的方法一来实验一下:\r\n",
    "``` python\r\n",
    "X = torch.randn(2, 3)\r\n",
    "Y = net(X)\r\n",
    "\r\n",
    "PATH = \"./net.pt\"\r\n",
    "torch.save(net.state_dict(), PATH)\r\n",
    "\r\n",
    "net2 = MLP()\r\n",
    "net2.load_state_dict(torch.load(PATH))\r\n",
    "Y2 = net2(X)\r\n",
    "Y2 == Y\r\n",
    "```\r\n",
    "输出：\r\n",
    "```\r\n",
    "tensor([[1],\r\n",
    "        [1]], dtype=torch.uint8)\r\n",
    "```\r\n",
    "\r\n",
    "因为这`net`和`net2`都有同样的模型参数，那么对同一个输入`X`的计算结果将会是一样的。上面的输出也验证了这一点。\r\n",
    "\r\n",
    "此外，还有一些其他使用场景，例如GPU与CPU之间的模型保存与读取、使用多块GPU的模型的存储等等，使用的时候可以参考[官方文档](https://pytorch.org/tutorials/beginner/saving_loading_models.html)。\r\n",
    "\r\n",
    "## 小结\r\n",
    "\r\n",
    "* 通过`save`函数和`load`函数可以很方便地读写`Tensor`。\r\n",
    "* 通过`save`函数和`load_state_dict`函数可以很方便地读写模型的参数。"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "243.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}