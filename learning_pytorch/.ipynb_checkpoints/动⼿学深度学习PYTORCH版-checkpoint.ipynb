{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e4a29ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d834895",
   "metadata": {},
   "source": [
    "## 2.2 数据操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e85f3d",
   "metadata": {},
   "source": [
    "### 2.2.1创建 `tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf6e338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9184e-39, 8.7245e-39, 9.2755e-39],\n",
       "        [8.9082e-39, 9.9184e-39, 8.4490e-39],\n",
       "        [9.6429e-39, 1.0653e-38, 1.0469e-38],\n",
       "        [4.2246e-39, 1.0378e-38, 9.6429e-39],\n",
       "        [9.2755e-39, 9.7346e-39, 1.0745e-38]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建⼀个5x3的未初始化的 Tensor\n",
    "x = torch.empty(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02a0e6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4496, 0.0336, 0.3475],\n",
       "        [0.5923, 0.2153, 0.8279],\n",
       "        [0.4446, 0.3286, 0.7065],\n",
       "        [0.7721, 0.6562, 0.4355],\n",
       "        [0.2791, 0.8337, 0.1916]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建⼀个5x3的随机初始化的 Tensor :\n",
    "x = torch.rand(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a1d4b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建⼀个5x3的long型全0的 Tensor :\n",
    "x = torch.zeros(5,3,dtype = torch.long)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44fb67e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5000, 2.0000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接创建\n",
    "x = torch.tensor([5.5,2])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1422cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过现有的tensor插件，会默认重用输入tensor的一些属性，如数据类型等，除非自定义数据类型\n",
    "x = x.new_ones(5, 3, dtype = torch.float64)\n",
    "# 返回的tensor默认具有相同的torch.dtype和torch.device\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ef12327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.8216e-02, -7.3244e-01, -7.2631e-01],\n",
       "        [ 2.2480e+00,  4.3884e-01, -3.3377e-01],\n",
       "        [ 1.9736e+00,  7.4171e-01,  2.4990e-01],\n",
       "        [ 6.7954e-01, -1.7059e+00,  1.8729e-01],\n",
       "        [ 3.0848e-01,  2.1080e-03,  1.2143e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 制定新的数据类型\n",
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cb6a2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]), torch.Size([5, 3]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以通过哦shape,size()来获取tensor的形状\n",
    "x.size(),x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ffa9c",
   "metadata": {},
   "source": [
    "> 注意：返回的torch.Size其实就是一个tuple, 支持所有tuple的操作。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a6485b",
   "metadata": {},
   "source": [
    "还有很多函数可以创建`Tensor`，去翻翻官方API就知道了，下表给了一些常用的作参考。\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|Tensor(*sizes)|基础构造函数|\n",
    "|tensor(data,)|类似np.array的构造函数|\n",
    "|ones(*sizes)|全1Tensor|\n",
    "|zeros(*sizes)|全0Tensor|\n",
    "|eye(*sizes)|对角线为1，其他为0|\n",
    "|arange(s,e,step)|从s到e，步长为step|\n",
    "|linspace(s,e,steps)|从s到e，均匀切分成steps份|\n",
    "|rand/randn(*sizes)|均匀/标准分布|\n",
    "|normal(mean,std)/uniform(from,to)|正态分布/均匀分布|\n",
    "|randperm(m)|随机排列|\n",
    "\n",
    "这些创建方法都可以在创建的时候指定数据类型dtype和存放device(cpu/gpu)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be29600a",
   "metadata": {},
   "source": [
    "## 2.2.2 操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82dc80c",
   "metadata": {},
   "source": [
    "### 加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f488341a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0719,  0.2269, -0.7175],\n",
      "        [ 2.5992,  0.6161, -0.2806],\n",
      "        [ 2.2163,  1.0230,  0.2533],\n",
      "        [ 1.0392, -1.0469,  0.3603],\n",
      "        [ 1.0761,  0.2050,  1.5645]])\n",
      "tensor([[ 0.0719,  0.2269, -0.7175],\n",
      "        [ 2.5992,  0.6161, -0.2806],\n",
      "        [ 2.2163,  1.0230,  0.2533],\n",
      "        [ 1.0392, -1.0469,  0.3603],\n",
      "        [ 1.0761,  0.2050,  1.5645]])\n",
      "tensor([[ 0.0719,  0.2269, -0.7175],\n",
      "        [ 2.5992,  0.6161, -0.2806],\n",
      "        [ 2.2163,  1.0230,  0.2533],\n",
      "        [ 1.0392, -1.0469,  0.3603],\n",
      "        [ 1.0761,  0.2050,  1.5645]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0719,  0.2269, -0.7175],\n",
       "        [ 2.5992,  0.6161, -0.2806],\n",
       "        [ 2.2163,  1.0230,  0.2533],\n",
       "        [ 1.0392, -1.0469,  0.3603],\n",
       "        [ 1.0761,  0.2050,  1.5645]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5,3)\n",
    "y = torch.rand(5,3)\n",
    "\n",
    "# 加法形式1：\n",
    "print(x + y )\n",
    "\n",
    "# 加法形式2：\n",
    "print(torch.add(x,y))\n",
    "#还可以指定输出\n",
    "res = torch.empty(5,3)\n",
    "torch.add(x,y,out=res)\n",
    "print(res)\n",
    "\n",
    "# 加法形式3：\n",
    "y.add_(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eeacc6",
   "metadata": {},
   "source": [
    "### 索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fb74683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2.0582, 1.2676, 1.2737]), tensor([2.0582, 1.2676, 1.2737]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 索引出来的结果与原数据共享内存，也即修改⼀个，另⼀个会跟着修改\n",
    "y = x[0,:]\n",
    "y += 1\n",
    "\n",
    "y,x[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26b553a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-31-a7743ddc4e52>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-31-a7743ddc4e52>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    y = torch.index_select(x,1,2,*,y)\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "y = torch.index_select(x,1,2,*,y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a1a506",
   "metadata": {},
   "source": [
    "除了常用的索引选择数据之外，PyTorch还提供了一些高级的选择函数:\n",
    "\n",
    "|函数|\t功能|\n",
    "|:---:|:---:|\n",
    "|index_select(input, dim, index)|在指定维度dim上选取，比如选取某些行、某些列|\n",
    "|masked_select(input, mask)|例子如上，a[a>0]，使用ByteTensor进行选取|\n",
    "|nonzero(input)|\t非0元素的下标|\n",
    "|gather(input, dim, index)|根据index，在dim维度上选取数据，输出的size与index一样|\n",
    "\n",
    "这里不详细介绍，用到了再查官方文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1017e56d",
   "metadata": {},
   "source": [
    "### 改变形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ee575ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]), torch.Size([15]), torch.Size([3, 5]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view()返回的新tensor与源tensor共享内存\n",
    "# view仅改变了观察方式\n",
    "y = x.view(15)\n",
    "z = x.view(-1,5)\n",
    "\n",
    "x.size(),y.size(),z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44fe7447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0582, 3.2676, 3.2737],\n",
      "        [4.2480, 2.4388, 1.6662],\n",
      "        [3.9736, 2.7417, 2.2499],\n",
      "        [2.6795, 0.2941, 2.1873],\n",
      "        [2.3085, 2.0021, 3.2143]])\n",
      "tensor([4.0582, 3.2676, 3.2737, 4.2480, 2.4388, 1.6662, 3.9736, 2.7417, 2.2499,\n",
      "        2.6795, 0.2941, 2.1873, 2.3085, 2.0021, 3.2143])\n"
     ]
    }
   ],
   "source": [
    "x += 1\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a83f6176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.0582,  2.2676,  2.2737],\n",
      "        [ 3.2480,  1.4388,  0.6662],\n",
      "        [ 2.9736,  1.7417,  1.2499],\n",
      "        [ 1.6795, -0.7059,  1.1873],\n",
      "        [ 1.3085,  1.0021,  2.2143]])\n",
      "tensor([4.0582, 3.2676, 3.2737, 4.2480, 2.4388, 1.6662, 3.9736, 2.7417, 2.2499,\n",
      "        2.6795, 0.2941, 2.1873, 2.3085, 2.0021, 3.2143])\n"
     ]
    }
   ],
   "source": [
    "# 如果我们想返回⼀个真正新的副本（即不共享内存）\n",
    "#Pytorch还提供了⼀个 reshape() 可以改变形状，但是此函数并不能保证返回的是其拷⻉，所以不推荐使⽤\n",
    "# 推荐先⽤ clone 创造⼀个副本然后再使⽤ view\n",
    "\n",
    "x_cp = x.clone().view(15)\n",
    "x -= 1\n",
    "\n",
    "print(x)\n",
    "print(x_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231b3c1",
   "metadata": {},
   "source": [
    "> 使用`clone`还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源`Tensor`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5197ae99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.4378]), -0.4377901256084442)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# item()将一个标量tensor转变成numpy\n",
    "# only one element tensors can be converted to Python scalars\n",
    "x = torch.randn(1)\n",
    "x,x.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0612c085",
   "metadata": {},
   "source": [
    "### 线性代数\n",
    "另外，PyTorch还支持一些线性函数，这里提一下，免得用起来的时候自己造轮子，具体用法参考官方文档。如下表所示：\n",
    "\n",
    "| 函数\t|功能|\n",
    "|:---:|:---:|\n",
    "|trace|\t对角线元素之和(矩阵的迹)|\n",
    "|diag|\t对角线元素|\n",
    "|triu/tril\t|矩阵的上三角/下三角，可指定偏移量|\n",
    "|mm/bmm\t|矩阵乘法，batch的矩阵乘法|\n",
    "|addmm/addbmm/addmv/addr/baddbmm..|\t矩阵运算|\n",
    "|t|转置|\n",
    "|dot/cross|\t内积/外积|\n",
    "|inverse\t|求逆矩阵|\n",
    "|svd\t|奇异值分解|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c4b254",
   "metadata": {},
   "source": [
    "## 2.2.3 广播机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737eddfe",
   "metadata": {},
   "source": [
    "前面我们看到如何对两个形状相同的`Tensor`做按元素运算。\n",
    "\n",
    "当对两个形状不同的`Tensor`按元素运算时，可能会触发广播（broadcasting）机制：\n",
    "\n",
    "先适当复制元素使这两个`Tensor`形状相同后再按元素运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3a37cab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1],\n",
       "         [2]]),\n",
       " tensor([[1, 2, 3, 4]]),\n",
       " tensor([[2, 3, 4, 5],\n",
       "         [3, 4, 5, 6]]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903fa7af",
   "metadata": {},
   "source": [
    "由于 x 和 y 分别是1⾏2列和3⾏1列的矩阵，如果要计算 x + y ，那么 x 中第⼀⾏的2个元素被⼴播\n",
    "（复制）到了第⼆⾏和第三⾏，⽽ y 中第⼀列的3个元素被⼴播（复制）到了第⼆列。如此，就可以对2\n",
    "个3⾏2列的矩阵按元素相加"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767ccef0",
   "metadata": {},
   "source": [
    "### 2.2.4运算的内存开销"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21750f8b",
   "metadata": {},
   "source": [
    "前⾯说了，索引、 view 是不会开辟新内存的，⽽像 y = x + y 这样的运算是会新开内存的，然后将 y 指向新内存为了演示这⼀点，我们可以使⽤Python⾃带的 id 函数：如果两个实例的ID⼀致，那么它们所对应的内存地址相同；反之则不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "516ae884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2629405413312, 2629405490496, -77184)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "y_id = id(y)\n",
    "y = y + x\n",
    "y_id_ = id(y)\n",
    "\n",
    "y_id,y_id_,y_id - y_id_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c873d",
   "metadata": {},
   "source": [
    "如果想指定结果到原来的 y 的内存，我们可以使⽤前⾯介绍的索引来进⾏替换操作。在下⾯的例⼦中，我们把 x + y 的结果通过 [:] 写进 y 对应的内存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "21e7c983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2629405488448, 2629405488448, 0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "y_id = id(y)\n",
    "y[:] = y + x\n",
    "y_id_ = id(y)\n",
    "\n",
    "y_id,y_id_,y_id - y_id_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c24707",
   "metadata": {},
   "source": [
    "我们还可以使⽤运算符全名函数中的 out 参数或者⾃加运算符 += (也即 add_() )达到上述效果，例如\n",
    "torch.add(x, y, out=y) 和 y += x ( y.add_(x) )。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "19d13420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2629405519360, 2629405519360, True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2])\n",
    "y = torch.tensor([3, 4])\n",
    "y_id = id(y)\n",
    "torch.add(x,y,out=y)# or y += x, y.add_(x)\n",
    "y_id_ = id(y)\n",
    "\n",
    "y_id,y_id_,y_id == y_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c53766",
   "metadata": {},
   "source": [
    "### 2.2.5 TENSOR 和NUMPY相互转换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285181a2",
   "metadata": {},
   "source": [
    "Notes: tensor和numpy中的数组也共享内存,（所以他们之间的转换很\n",
    "快），改变其中⼀个时另⼀个也会改变！！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a112b0c",
   "metadata": {},
   "source": [
    "#### numpy中的array转换成tensor\n",
    "需要注意的是，此⽅法总是会进⾏数据拷⻉（就会消耗更多的时间和空间），所以返回的 Tensor 和原来的数\n",
    "据不再共享内存。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ecbc7084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 2., 2.]), tensor([2., 2., 2.], dtype=torch.float64))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones(3)\n",
    "b = torch.from_numpy(a)\n",
    "\n",
    "a += 1\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f0ce10ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 2., 2.]), tensor([2., 2., 2.], dtype=torch.float64))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 法2\n",
    "b = torch.tensor(a)\n",
    "\n",
    "a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aa33dd",
   "metadata": {},
   "source": [
    "所有在CPU上的 Tensor （除了 CharTensor ）都⽀持与NumPy数组相互转换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73309795",
   "metadata": {},
   "source": [
    "#### tensor转numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ca9a7012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2.]), array([2., 2.], dtype=float32))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(2)\n",
    "b = a.numpy()\n",
    "\n",
    "a += 1\n",
    "\n",
    "a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762e5276",
   "metadata": {},
   "source": [
    "### 2.2.6 TENSOR ON GPU\n",
    "用方法`to()`可以将`Tensor`在CPU和GPU（需要硬件支持）之间相互移动。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4a8c1bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下代码只有在PyTorch GPU版本上才会执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa408ea9",
   "metadata": {},
   "source": [
    "## 2.3 ⾃动求梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4527ce",
   "metadata": {},
   "source": [
    "在深度学习中，我们经常需要对函数求梯度（gradient）。PyTorch提供的autograd 包能够根据输⼊\n",
    "和前向传播过程⾃动构建计算图，并执⾏反向传播。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49837381",
   "metadata": {},
   "source": [
    "### 2.3.1 概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d3c669",
   "metadata": {},
   "source": [
    "上一节介绍的`Tensor`是这个包的核心类，如果将其属性`.requires_grad`设置为`True`，它将开始追踪(track)在其上的所有操作（这样就可以利用链式法则进行梯度传播了）。完成计算后，可以调用`.backward()`来完成所有梯度计算。此`Tensor`的梯度将累积到`.grad`属性中。\n",
    "> 注意在`y.backward()`时，如果`y`是标量，则不需要为`backward()`传入任何参数；否则，需要传入一个与`y`同形的`Tensor`。解释见 2.3.2 节。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf68fb02",
   "metadata": {},
   "source": [
    "如果不想要被继续追踪，可以调⽤ .detach() 将其从追踪记录中分离出来，这样就可以防⽌将来的计算被追踪，这样梯度就传不过去了。此外，还可以⽤ with torch.no_grad() 将不想被追踪的操作代码块包裹起来，这种⽅法在评估模型的时候很常⽤，因为在评估模型时，我们并不需要计算可训练参数（ requires_grad=True ）的梯度。\n",
    "\n",
    "Function 是另外⼀个很᯿要的类。 Tensor 和 Function 互相结合就可以构建⼀个记录有整个计算过程的有向⽆环图（DAG）。每个 Tensor 都有⼀个 .grad_fn 属性，该属性即创建该 Tensor 的Function , 就是说该 Tensor 是不是通过某些运算得到的，若是，则 grad_fn 返回⼀个与这些运算相关的对象，否则是None。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da280bd",
   "metadata": {},
   "source": [
    "## 2.3.2 `Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "dad7f99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1.],\n",
       "         [1., 1.]], requires_grad=True),\n",
       " None)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 即把x当做变量\n",
    "x = torch.ones(2,2,requires_grad = True)\n",
    "x,x.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "683e8836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 3.],\n",
       "         [3., 3.]], grad_fn=<AddBackward0>),\n",
       " <AddBackward0 at 0x26434c83be0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 做一下运算\n",
    "\n",
    "y = x +2\n",
    "y,y.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83ad789",
   "metadata": {},
   "source": [
    "注意x是直接创建的，所以它没有 grad_fn , ⽽y是通过⼀个加法操作创建的，所以它有⼀个为\n",
    "<AddBackward> 的 grad_fn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f4703ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "# 像x这种直接创建的称为叶⼦节点，叶⼦节点对应的 grad_fn 是 None 。\n",
    "print(x.is_leaf,y.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "927d6364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "<MulBackward0 object at 0x0000026434C7FA90>\n",
      "\n",
      "tensor(27., grad_fn=<MeanBackward0>)\n",
      "<MeanBackward0 object at 0x0000026434C882B0>\n"
     ]
    }
   ],
   "source": [
    "# 可以从grad_fn看出本次的操作 or 运算\n",
    "z = y ** 2 * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z)\n",
    "print(z.grad_fn)\n",
    "print()\n",
    "print(out)\n",
    "print(out.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee7e08",
   "metadata": {},
   "source": [
    "通过`.requires_grad_()`来用in-place的方式改变`requires_grad`属性：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7488a27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x0000026433C527C0>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2) # 缺失情况下默认 requires_grad = False\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad) # False\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad) # True\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275622ef",
   "metadata": {},
   "source": [
    "## 2.3.3 梯度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "48fb7aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  因为 out 是⼀个标量，所以调⽤ backward() 时不需要指定求导变量\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e336569",
   "metadata": {},
   "source": [
    " 我们来看看`out`关于`x`的梯度 $\\frac{d(out)}{dx}$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "35e07319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adf9274",
   "metadata": {},
   "source": [
    "我们令`out`为 $o$ , 因为\n",
    "$$\n",
    "o=\\frac14\\sum_{i=1}^4z_i=\\frac14\\sum_{i=1}^43(x_i+2)^2\n",
    "$$\n",
    "所以\n",
    "$$\n",
    "\\frac{\\partial{o}}{\\partial{x_i}}\\bigr\\rvert_{x_i=1}=\\frac{9}{2}=4.5\n",
    "$$\n",
    "所以上面的输出是正确的。\n",
    "\n",
    "数学上，如果有一个函数值和自变量都为向量的函数 $\\vec{y}=f(\\vec{x})$, 那么 $\\vec{y}$ 关于 $\\vec{x}$ 的梯度就是一个雅可比矩阵（Jacobian matrix）:\n",
    "$$\n",
    "J=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\n",
    "$$\n",
    "而``torch.autograd``这个包就是用来计算一些雅克比矩阵的乘积的。例如，如果 $v$ 是一个标量函数的 $l=g\\left(\\vec{y}\\right)$ 的梯度：\n",
    "$$\n",
    "v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)\n",
    "$$\n",
    "那么根据链式法则我们有 $l$ 关于 $\\vec{x}$ 的雅克比矩阵就为:\n",
    "$$\n",
    "v J=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right) \\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial x_{1}} & \\cdots & \\frac{\\partial l}{\\partial x_{n}}\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "注意：grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "38bbc223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#再来一次反向传播，注意grad是累加的\n",
    "\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "\n",
    "out3 = x.sum()\n",
    "x.grad.data.zero_()\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4906695",
   "metadata": {},
   "source": [
    "\n",
    "> 现在我们解释2.3.1节留下的问题，为什么在`y.backward()`时，如果`y`是标量，则不需要为`backward()`传入任何参数；否则，需要传入一个与`y`同形的`Tensor`?\n",
    "简单来说就是为了避免向量（甚至更高维张量）对张量求导，而转换成标量对张量求导。举个例子，假设形状为 `m x n` 的矩阵 X 经过运算得到了 `p x q` 的矩阵 Y，Y 又经过运算得到了 `s x t` 的矩阵 Z。那么按照前面讲的规则，dZ/dY 应该是一个 `s x t x p x q` 四维张量，dY/dX 是一个 `p x q x m x n`的四维张量。问题来了，怎样反向传播？怎样将两个四维张量相乘？？？这要怎么乘？？？就算能解决两个四维张量怎么乘的问题，四维和三维的张量又怎么乘？导数的导数又怎么求，这一连串的问题，感觉要疯掉…… \n",
    "为了避免这个问题，我们**不允许张量对张量求导，只允许标量对张量求导，求导结果是和自变量同形的张量**。所以必要时我们要**把张量通过将所有张量的元素加权求和的方式转换为标量**，举个例子，假设`y`由自变量`x`计算而来，`w`是和`y`同形的张量，则`y.backward(w)`的含义是：先计算`l = torch.sum(y * w)`，则`l`是个标量，然后求`l`对自变量`x`的导数。\n",
    "[参考](https://zhuanlan.zhihu.com/p/29923090)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b8fc6064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 4.],\n",
       "        [6., 8.]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only Tensors of floating point and complex dtype can require gradients\n",
    "x = torch.tensor([1.,2.,3.,4.],requires_grad=True)\n",
    "y = 2 * x\n",
    "z = y.view(2,2)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab100a2",
   "metadata": {},
   "source": [
    "现在 z 不是⼀个标量，所以在调⽤ backward 时需要传⼊⼀个和z同形的权重向量进⾏加权求和得到\n",
    "⼀个标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f1e9b09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000, 0.2000, 0.0200, 0.0020])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([[1.0, 0.1], [0.01, 0.001]], dtype=torch.float)\n",
    "z.backward(v)\n",
    "print(x.grad)\n",
    "# 注意， x.grad 是和 x 同形的张量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f49dff4",
   "metadata": {},
   "source": [
    "**为什么**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1a18e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for question\n",
    "l = torch.sum(v * z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9003dc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(1., grad_fn=<PowBackward0>) True\n",
      "tensor(1.) False\n",
      "tensor(2., grad_fn=<AddBackward0>) True\n",
      "tensor(2.)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-151-575a1166fcfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 只算了y1 part\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0my2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# 再来卡看中断梯度的例子：\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y1 = x ** 2\n",
    "with torch.no_grad():\n",
    "    y2 = x ** 3\n",
    "y3 = y1 + y2\n",
    "\n",
    "print(x.requires_grad)\n",
    "print(y1, y1.requires_grad)\n",
    "print(y2, y2.requires_grad)\n",
    "print(y3, y3.requires_grad)\n",
    "\n",
    "y3.backward()\n",
    "print(x.grad) # 只算了y1 part\n",
    "\n",
    "y2.backward() # 因为requires_grad = false，所以报错"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc1c37",
   "metadata": {},
   "source": [
    "此外，如果我们想要修改 tensor 的数值，但是⼜不希望被 autograd 记录（即不会影响反向传播），\n",
    "那么我么可以对 tensor.data 进⾏操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7890a8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1,requires_grad=True)\n",
    "print(x.data) # 还是⼀个tensor\n",
    "print(x.data.requires_grad) # 但是已经是独⽴于计算图之外\n",
    "y = 2 * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "y.backward()\n",
    "print(x) # 更改data的值也会影响tensor的值\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97ac4bf",
   "metadata": {},
   "source": [
    "## 3.1 线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c8c2b",
   "metadata": {},
   "source": [
    "线性回归输出是⼀个连续值，因此适⽤于回归问题。回归问题在实际中很常⻅，如预测房屋价格、⽓\n",
    "温、销售额等连续值的问题。与回归问题不同，分类问题中模型的最终输出是⼀个离散值。我们所说的\n",
    "图像分类、垃圾邮件识别、疾病检测等输出为离散值的问题都属于分类问题的范畴。softmax回归则适\n",
    "⽤于分类问题。\n",
    "由于线性回归和softmax回归都是单层神经⽹络，它们涉及的概念和技术同样适⽤于⼤多数的深度学习\n",
    "模型。我们⾸先以线性回归为例，介绍⼤多数深度学习模型的基本要素和表示⽅法。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ecdf5fda",
   "metadata": {},
   "source": [
    "### 3.1.1 线性回归的基本要素\n",
    "以⼀个简单的房屋价格预测作为例⼦来解释线性回归的基本要素。这个应⽤的⽬标是预测⼀栋房⼦\n",
    "的售出价格（元）。我们知道这个价格取决于很多因素，如房屋状况、地段、市场⾏情等。为了简单起\n",
    "⻅，这⾥我们假设价格只取决于房屋状况的两个因素，即⾯积（平⽅⽶）和房龄（年）。接下来我们希\n",
    "望探索价格与这两个因素的具体关系\n",
    " \n",
    "### 3.1.1.1 模型定义\n",
    "\n",
    "设房屋的面积为 $x_1$，房龄为 $x_2$，售出价格为 $y$。我们需要建立基于输入 $x_1$ 和 $x_2$ 来计算输出 $y$ 的表达式，也就是模型（model）。顾名思义，线性回归假设输出与各个输入之间是线性关系：\n",
    "$$\n",
    "\\hat{y} = x_1 w_1 + x_2 w_2 + b\n",
    "$$\n",
    "其中 $w_1$ 和 $w_2$ 是权重（weight），$b$ 是偏差（bias），且均为标量。它们是线性回归模型的参数（parameter）。模型输出 $\\hat{y}$ 是线性回归对真实价格 $y$ 的预测或估计。我们通常允许它们之间有一定误差。\n",
    "\n",
    "\n",
    "### 3.1.1.2 模型训练\n",
    "\n",
    "接下来我们需要通过数据来寻找特定的模型参数值，使模型在数据上的误差尽可能小。这个过程叫作模型训练（model training）。下面我们介绍模型训练所涉及的3个要素。\n",
    "\n",
    "#### (1) 训练数据\n",
    "我们通常收集一系列的真实数据，例如多栋房屋的真实售出价格和它们对应的面积和房龄。我们希望在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语里，该数据集被称为训练数据集（training data set）或训练集（training set），一栋房屋被称为一个样本（sample），其真实售出价格叫作标签（label），用来预测标签的两个因素叫作特征（feature）。特征用来表征样本的特点。\n",
    "\n",
    "假设我们采集的样本数为 $n$，索引为 $i$ 的样本的特征为 $x_1^{(i)}$ 和 $x_2^{(i)}$，标签为 $y^{(i)}$。对于索引为 $i$ 的房屋，线性回归模型的房屋价格预测表达式为\n",
    "$$\n",
    "\\hat{y}^{(i)} = x_1^{(i)} w_1 + x_2^{(i)} w_2 + b\n",
    "$$\n",
    "\n",
    "#### (2) 损失函数\n",
    "\n",
    "在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。它在评估索引为 $i$ 的样本误差的表达式为\n",
    "\n",
    "$$\\ell^{(i)}(w_1, w_2, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2$$\n",
    "\n",
    "其中常数 $\\frac 1 2$ 使对平方项求导后的常数系数为1，这样在形式上稍微简单一些。显然，误差越小表示预测价格与真实价格越相近，且当二者相等时误差为0。给定训练数据集，这个误差只与模型参数相关，因此我们将它记为以模型参数为参数的函数。在机器学习里，将衡量误差的函数称为损失函数（loss function）。这里使用的平方误差函数也称为平方损失（square loss）。\n",
    "\n",
    "通常，我们用训练数据集中所有样本误差的平均来衡量模型预测的质量，即\n",
    "\n",
    "$$\n",
    "\\ell(w_1, w_2, b) =\\frac{1}{n} \\sum_{i=1}^n \\ell^{(i)}(w_1, w_2, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right)^2\n",
    "$$\n",
    "\n",
    "在模型训练中，我们希望找出一组模型参数，记为 $w_1^*, w_2^*, b^*$，来使训练样本平均损失最小：\n",
    "\n",
    "$$\n",
    "w_1^*, w_2^*, b^* = \\underset{w_1, w_2, b}{\\arg\\min} \\ell(w_1, w_2, b)\n",
    "$$\n",
    "\n",
    "\n",
    "#### (3) 优化算法\n",
    "\n",
    "当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。\n",
    "\n",
    "在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）$\\mathcal{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。\n",
    "\n",
    "在训练本节讨论的线性回归模型的过程中，模型的每个参数将作如下迭代：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w_1 &\\leftarrow w_1 -   \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\frac{ \\partial \\ell^{(i)}(w_1, w_2, b)  }{\\partial w_1} = w_1 -   \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}x_1^{(i)} \\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right),\\\\\n",
    "w_2 &\\leftarrow w_2 -   \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\frac{ \\partial \\ell^{(i)}(w_1, w_2, b)  }{\\partial w_2} = w_2 -   \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}x_2^{(i)} \\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right),\\\\\n",
    "b &\\leftarrow b -   \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\frac{ \\partial \\ell^{(i)}(w_1, w_2, b)  }{\\partial b} = b -   \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}\\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "在上式中，$|\\mathcal{B}|$ 代表每个小批量中的样本个数（批量大小，batch size），$\\eta$ 称作学习率（learning rate）并取正数。需要强调的是，这里的批量大小和学习率的值是人为设定的，并不是通过模型训练学出的，因此叫作超参数（hyperparameter）。我们通常所说的“调参”指的正是调节超参数，例如通过反复试错来找到超参数合适的值。在少数情况下，超参数也可以通过模型训练学出。本书对此类情况不做讨论。\n",
    "\n",
    "### 3.1.1.3 模型预测\n",
    "模型训练完成后，我们将模型参数 $w_1, w_2, b$ 在优化算法停止时的值分别记作 $\\hat{w}_1, \\hat{w}_2, \\hat{b}$。注意，这里我们得到的并不一定是最小化损失函数的最优解 $w_1^*, w_2^*, b^*$，而是对最优解的一个近似。然后，我们就可以使用学出的线性回归模型 $x_1 \\hat{w}_1 + x_2 \\hat{w}_2 + \\hat{b}$ 来估算训练数据集以外任意一栋面积（平方米）为$x_1$、房龄（年）为$x_2$的房屋的价格了。这里的估算也叫作模型预测、模型推断或模型测试。\n",
    "\n",
    "\n",
    "## 3.1.2 线性回归的表示方法\n",
    "\n",
    "我们已经阐述了线性回归的模型表达式、训练和预测。下面我们解释线性回归与神经网络的联系，以及线性回归的矢量计算表达式。\n",
    "\n",
    "### 3.1.2.1 神经网络图\n",
    "\n",
    "在深度学习中，我们可以使用神经网络图直观地表现模型结构。为了更清晰地展示线性回归作为神经网络的结构，图3.1使用神经网络图表示本节中介绍的线性回归模型。神经网络图隐去了模型参数权重和偏差。\n",
    "\n",
    "<div align=center>\n",
    "    ![image.png](attachment:image.png)\n",
    "</div>\n",
    "<div align=center>图3.1 线性回归是一个单层神经网络</div>\n",
    "\n",
    "\n",
    "在图3.1所示的神经网络中，输入分别为 $x_1$ 和 $x_2$，因此输入层的输入个数为2。输入个数也叫特征数或特征向量维度。图3.1中网络的输出为 $o$，输出层的输出个数为1。需要注意的是，我们直接将图3.1中神经网络的输出 $o$ 作为线性回归的输出，即 $\\hat{y} = o$。由于输入层并不涉及计算，按照惯例，图3.1所示的神经网络的层数为1。所以，线性回归是一个单层神经网络。输出层中负责计算 $o$ 的单元又叫神经元。在线性回归中，$o$ 的计算依赖于 $x_1$ 和 $x_2$。也就是说，输出层中的神经元和输入层中各个输入完全连接。因此，这里的输出层又叫全连接层（fully-connected layer）或稠密层（dense layer）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9392278c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
